{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O8QusriF8XiB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "19u3MU7YmLsT",
        "outputId": "f2a5d248-4bb4-406e-ab62-4cd2405e7f12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0    842302         M        17.99         10.38          122.80     1001.0   \n",
              "1    842517         M        20.57         17.77          132.90     1326.0   \n",
              "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3  84348301         M        11.42         20.38           77.58      386.1   \n",
              "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0  ...          17.33           184.60      2019.0            0.1622   \n",
              "1  ...          23.41           158.80      1956.0            0.1238   \n",
              "2  ...          25.53           152.50      1709.0            0.1444   \n",
              "3  ...          26.50            98.87       567.7            0.2098   \n",
              "4  ...          16.67           152.20      1575.0            0.1374   \n",
              "\n",
              "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0             0.6656           0.7119                0.2654          0.4601   \n",
              "1             0.1866           0.2416                0.1860          0.2750   \n",
              "2             0.4245           0.4504                0.2430          0.3613   \n",
              "3             0.8663           0.6869                0.2575          0.6638   \n",
              "4             0.2050           0.4000                0.1625          0.2364   \n",
              "\n",
              "   fractal_dimension_worst  Unnamed: 32  \n",
              "0                  0.11890          NaN  \n",
              "1                  0.08902          NaN  \n",
              "2                  0.08758          NaN  \n",
              "3                  0.17300          NaN  \n",
              "4                  0.07678          NaN  \n",
              "\n",
              "[5 rows x 33 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0ab18ba4-3749-4697-bf4d-ca67afbb57fd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ab18ba4-3749-4697-bf4d-ca67afbb57fd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0ab18ba4-3749-4697-bf4d-ca67afbb57fd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0ab18ba4-3749-4697-bf4d-ca67afbb57fd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-405eca70-64a5-4c1d-ac46-57094fe447b2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-405eca70-64a5-4c1d-ac46-57094fe447b2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-405eca70-64a5-4c1d-ac46-57094fe447b2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['id', 'Unnamed: 32'], axis=1, inplace=True)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "lqnsYZHnmLpc",
        "outputId": "1f20ceb1-d125-4ad3-b435-d3c76d129328"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0         M        17.99         10.38          122.80     1001.0   \n",
              "1         M        20.57         17.77          132.90     1326.0   \n",
              "2         M        19.69         21.25          130.00     1203.0   \n",
              "3         M        11.42         20.38           77.58      386.1   \n",
              "4         M        20.29         14.34          135.10     1297.0   \n",
              "\n",
              "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0          0.11840           0.27760          0.3001              0.14710   \n",
              "1          0.08474           0.07864          0.0869              0.07017   \n",
              "2          0.10960           0.15990          0.1974              0.12790   \n",
              "3          0.14250           0.28390          0.2414              0.10520   \n",
              "4          0.10030           0.13280          0.1980              0.10430   \n",
              "\n",
              "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
              "0         0.2419  ...         25.38          17.33           184.60   \n",
              "1         0.1812  ...         24.99          23.41           158.80   \n",
              "2         0.2069  ...         23.57          25.53           152.50   \n",
              "3         0.2597  ...         14.91          26.50            98.87   \n",
              "4         0.1809  ...         22.54          16.67           152.20   \n",
              "\n",
              "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
              "0      2019.0            0.1622             0.6656           0.7119   \n",
              "1      1956.0            0.1238             0.1866           0.2416   \n",
              "2      1709.0            0.1444             0.4245           0.4504   \n",
              "3       567.7            0.2098             0.8663           0.6869   \n",
              "4      1575.0            0.1374             0.2050           0.4000   \n",
              "\n",
              "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
              "0                0.2654          0.4601                  0.11890  \n",
              "1                0.1860          0.2750                  0.08902  \n",
              "2                0.2430          0.3613                  0.08758  \n",
              "3                0.2575          0.6638                  0.17300  \n",
              "4                0.1625          0.2364                  0.07678  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-44f703b2-5ead-4229-b36e-b6f254b5812c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.3001</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>...</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44f703b2-5ead-4229-b36e-b6f254b5812c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-44f703b2-5ead-4229-b36e-b6f254b5812c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-44f703b2-5ead-4229-b36e-b6f254b5812c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-23f0add6-3b13-451b-9703-fc207ebb9bcf\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23f0add6-3b13-451b-9703-fc207ebb9bcf')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-23f0add6-3b13-451b-9703-fc207ebb9bcf button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size=0.2)\n",
        "X_train.shape, X_test.shape, Y_train.shape, Y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXCtZ7IamLmi",
        "outputId": "5962d556-3c4e-4854-ef72-dd3c6817f3ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((455, 30), (114, 30), (455,), (114,))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "YoFbniy1mLj1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "Y_train = le.fit_transform(Y_train)\n",
        "Y_test = le.transform(Y_test)"
      ],
      "metadata": {
        "id": "xRuijPEtmLgu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test_tensor = torch.from_numpy(X_test.astype(np.float32))\n",
        "Y_train_tensor = torch.from_numpy(Y_train.astype(np.float32))\n",
        "Y_test_tensor = torch.from_numpy(Y_test.astype(np.float32))"
      ],
      "metadata": {
        "id": "AJJE1J61mLd1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor.shape, X_test_tensor.shape, Y_train_tensor.shape, Y_test_tensor.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTbojZi7mLa3",
        "outputId": "f524dc5e-d68c-45bf-ad66-51c84cc67e1f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([455, 30]),\n",
              " torch.Size([114, 30]),\n",
              " torch.Size([455]),\n",
              " torch.Size([114]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "  def __init__(self, num_features):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(num_features, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.linear(x)\n",
        "    return self.sigmoid(z)\n",
        "\n",
        "  def loss(self, y_pred, Y):\n",
        "    epsilon = 1e-7\n",
        "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
        "    loss = Y * torch.log(y_pred) + (1 - Y) * torch.log(1 - y_pred)\n",
        "    return -torch.mean(loss)"
      ],
      "metadata": {
        "id": "8diVtT7PmLXy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01\n",
        "epochs = 1000"
      ],
      "metadata": {
        "id": "fFR6sK50mLUs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_IBh65cgn-cJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using custom loss function and optimizer"
      ],
      "metadata": {
        "id": "WD1DD9TSoeTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyNN(X_train_tensor.shape[1])\n",
        "\n",
        "for i in range(epochs):\n",
        "  # forward pass\n",
        "  y_pred = model(X_train_tensor)\n",
        "\n",
        "  # loss calculation\n",
        "  loss = model.loss(y_pred, Y_train_tensor)\n",
        "  print(f'Epoch: {i + 1} | Loss: {loss.item()}', end='--->')\n",
        "  if (i + 1) % 100 == 0:\n",
        "    print()\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # weight update\n",
        "  with torch.no_grad():\n",
        "    model.linear.weight -= lr * model.linear.weight.grad\n",
        "    model.linear.bias -= lr * model.linear.bias.grad\n",
        "\n",
        "  # zero the gradients\n",
        "  model.linear.weight.grad.zero_()\n",
        "  model.linear.bias.grad.zero_()\n",
        "\n",
        "model.linear.weight, model.linear.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzQZjXdGmLRs",
        "outputId": "5092105d-e775-4395-9634-f8a1e520c416"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss: 0.7296302318572998--->Epoch: 2 | Loss: 0.7290014624595642--->Epoch: 3 | Loss: 0.7283914089202881--->Epoch: 4 | Loss: 0.7277989387512207--->Epoch: 5 | Loss: 0.7272233963012695--->Epoch: 6 | Loss: 0.7266639471054077--->Epoch: 7 | Loss: 0.7261197566986084--->Epoch: 8 | Loss: 0.7255903482437134--->Epoch: 9 | Loss: 0.7250745892524719--->Epoch: 10 | Loss: 0.7245721220970154--->Epoch: 11 | Loss: 0.7240822315216064--->Epoch: 12 | Loss: 0.7236045002937317--->Epoch: 13 | Loss: 0.7231382131576538--->Epoch: 14 | Loss: 0.7226828932762146--->Epoch: 15 | Loss: 0.7222379446029663--->Epoch: 16 | Loss: 0.7218030095100403--->Epoch: 17 | Loss: 0.7213776111602783--->Epoch: 18 | Loss: 0.7209613919258118--->Epoch: 19 | Loss: 0.7205538153648376--->Epoch: 20 | Loss: 0.7201545834541321--->Epoch: 21 | Loss: 0.7197633385658264--->Epoch: 22 | Loss: 0.7193797826766968--->Epoch: 23 | Loss: 0.719003438949585--->Epoch: 24 | Loss: 0.7186342477798462--->Epoch: 25 | Loss: 0.7182717323303223--->Epoch: 26 | Loss: 0.7179156541824341--->Epoch: 27 | Loss: 0.7175655961036682--->Epoch: 28 | Loss: 0.7172216773033142--->Epoch: 29 | Loss: 0.7168833613395691--->Epoch: 30 | Loss: 0.7165505886077881--->Epoch: 31 | Loss: 0.7162231206893921--->Epoch: 32 | Loss: 0.715900719165802--->Epoch: 33 | Loss: 0.7155832052230835--->Epoch: 34 | Loss: 0.7152704000473022--->Epoch: 35 | Loss: 0.7149621844291687--->Epoch: 36 | Loss: 0.7146583199501038--->Epoch: 37 | Loss: 0.7143586874008179--->Epoch: 38 | Loss: 0.7140631675720215--->Epoch: 39 | Loss: 0.7137715816497803--->Epoch: 40 | Loss: 0.7134838700294495--->Epoch: 41 | Loss: 0.7131997346878052--->Epoch: 42 | Loss: 0.7129193544387817--->Epoch: 43 | Loss: 0.7126423716545105--->Epoch: 44 | Loss: 0.7123687267303467--->Epoch: 45 | Loss: 0.7120983004570007--->Epoch: 46 | Loss: 0.7118311524391174--->Epoch: 47 | Loss: 0.7115669846534729--->Epoch: 48 | Loss: 0.7113059759140015--->Epoch: 49 | Loss: 0.7110476493835449--->Epoch: 50 | Loss: 0.7107921838760376--->Epoch: 51 | Loss: 0.7105395793914795--->Epoch: 52 | Loss: 0.7102895975112915--->Epoch: 53 | Loss: 0.7100421786308289--->Epoch: 54 | Loss: 0.7097973227500916--->Epoch: 55 | Loss: 0.7095550298690796--->Epoch: 56 | Loss: 0.7093150615692139--->Epoch: 57 | Loss: 0.7090774774551392--->Epoch: 58 | Loss: 0.7088421583175659--->Epoch: 59 | Loss: 0.7086091637611389--->Epoch: 60 | Loss: 0.7083783149719238--->Epoch: 61 | Loss: 0.7081496119499207--->Epoch: 62 | Loss: 0.7079230546951294--->Epoch: 63 | Loss: 0.70769864320755--->Epoch: 64 | Loss: 0.7074760794639587--->Epoch: 65 | Loss: 0.7072555422782898--->Epoch: 66 | Loss: 0.7070370316505432--->Epoch: 67 | Loss: 0.7068203687667847--->Epoch: 68 | Loss: 0.7066054940223694--->Epoch: 69 | Loss: 0.7063924670219421--->Epoch: 70 | Loss: 0.7061813473701477--->Epoch: 71 | Loss: 0.7059719562530518--->Epoch: 72 | Loss: 0.7057642340660095--->Epoch: 73 | Loss: 0.7055583000183105--->Epoch: 74 | Loss: 0.7053539752960205--->Epoch: 75 | Loss: 0.7051512598991394--->Epoch: 76 | Loss: 0.704950213432312--->Epoch: 77 | Loss: 0.7047507166862488--->Epoch: 78 | Loss: 0.7045528292655945--->Epoch: 79 | Loss: 0.7043564915657043--->Epoch: 80 | Loss: 0.7041616439819336--->Epoch: 81 | Loss: 0.7039682865142822--->Epoch: 82 | Loss: 0.7037762403488159--->Epoch: 83 | Loss: 0.7035859823226929--->Epoch: 84 | Loss: 0.7033968567848206--->Epoch: 85 | Loss: 0.7032092809677124--->Epoch: 86 | Loss: 0.7030230164527893--->Epoch: 87 | Loss: 0.702838122844696--->Epoch: 88 | Loss: 0.7026546597480774--->Epoch: 89 | Loss: 0.7024725079536438--->Epoch: 90 | Loss: 0.7022916078567505--->Epoch: 91 | Loss: 0.7021119594573975--->Epoch: 92 | Loss: 0.7019338011741638--->Epoch: 93 | Loss: 0.7017567157745361--->Epoch: 94 | Loss: 0.7015808820724487--->Epoch: 95 | Loss: 0.7014063596725464--->Epoch: 96 | Loss: 0.7012329697608948--->Epoch: 97 | Loss: 0.7010608911514282--->Epoch: 98 | Loss: 0.7008898854255676--->Epoch: 99 | Loss: 0.7007200717926025--->Epoch: 100 | Loss: 0.7005515694618225--->\n",
            "Epoch: 101 | Loss: 0.7003839612007141--->Epoch: 102 | Loss: 0.7002176642417908--->Epoch: 103 | Loss: 0.7000524401664734--->Epoch: 104 | Loss: 0.699888288974762--->Epoch: 105 | Loss: 0.6997252702713013--->Epoch: 106 | Loss: 0.699563205242157--->Epoch: 107 | Loss: 0.699402391910553--->Epoch: 108 | Loss: 0.6992425918579102--->Epoch: 109 | Loss: 0.6990838050842285--->Epoch: 110 | Loss: 0.6989261507987976--->Epoch: 111 | Loss: 0.6987693905830383--->Epoch: 112 | Loss: 0.698613703250885--->Epoch: 113 | Loss: 0.6984589695930481--->Epoch: 114 | Loss: 0.6983053088188171--->Epoch: 115 | Loss: 0.6981525421142578--->Epoch: 116 | Loss: 0.6980008482933044--->Epoch: 117 | Loss: 0.6978500485420227--->Epoch: 118 | Loss: 0.6977002620697021--->Epoch: 119 | Loss: 0.6975513100624084--->Epoch: 120 | Loss: 0.6974033713340759--->Epoch: 121 | Loss: 0.6972563862800598--->Epoch: 122 | Loss: 0.6971102356910706--->Epoch: 123 | Loss: 0.696965217590332--->Epoch: 124 | Loss: 0.6968207955360413--->Epoch: 125 | Loss: 0.6966775059700012--->Epoch: 126 | Loss: 0.6965349912643433--->Epoch: 127 | Loss: 0.6963933110237122--->Epoch: 128 | Loss: 0.6962525844573975--->Epoch: 129 | Loss: 0.6961126923561096--->Epoch: 130 | Loss: 0.6959736347198486--->Epoch: 131 | Loss: 0.6958354115486145--->Epoch: 132 | Loss: 0.695698082447052--->Epoch: 133 | Loss: 0.6955615878105164--->Epoch: 134 | Loss: 0.6954259276390076--->Epoch: 135 | Loss: 0.6952910423278809--->Epoch: 136 | Loss: 0.695156991481781--->Epoch: 137 | Loss: 0.6950235962867737--->Epoch: 138 | Loss: 0.6948912143707275--->Epoch: 139 | Loss: 0.6947595477104187--->Epoch: 140 | Loss: 0.6946286559104919--->Epoch: 141 | Loss: 0.6944985389709473--->Epoch: 142 | Loss: 0.6943691372871399--->Epoch: 143 | Loss: 0.6942405104637146--->Epoch: 144 | Loss: 0.6941127181053162--->Epoch: 145 | Loss: 0.693985641002655--->Epoch: 146 | Loss: 0.693859338760376--->Epoch: 147 | Loss: 0.6937337517738342--->Epoch: 148 | Loss: 0.6936088800430298--->Epoch: 149 | Loss: 0.6934847831726074--->Epoch: 150 | Loss: 0.6933614015579224--->Epoch: 151 | Loss: 0.6932386755943298--->Epoch: 152 | Loss: 0.6931167840957642--->Epoch: 153 | Loss: 0.6929954290390015--->Epoch: 154 | Loss: 0.6928749084472656--->Epoch: 155 | Loss: 0.6927549242973328--->Epoch: 156 | Loss: 0.6926357746124268--->Epoch: 157 | Loss: 0.6925172805786133--->Epoch: 158 | Loss: 0.6923993825912476--->Epoch: 159 | Loss: 0.6922823190689087--->Epoch: 160 | Loss: 0.6921657919883728--->Epoch: 161 | Loss: 0.6920499205589294--->Epoch: 162 | Loss: 0.6919347643852234--->Epoch: 163 | Loss: 0.6918202638626099--->Epoch: 164 | Loss: 0.6917064785957336--->Epoch: 165 | Loss: 0.6915931701660156--->Epoch: 166 | Loss: 0.6914806365966797--->Epoch: 167 | Loss: 0.6913686394691467--->Epoch: 168 | Loss: 0.6912574172019958--->Epoch: 169 | Loss: 0.6911466717720032--->Epoch: 170 | Loss: 0.6910366415977478--->Epoch: 171 | Loss: 0.6909272074699402--->Epoch: 172 | Loss: 0.6908183693885803--->Epoch: 173 | Loss: 0.690710186958313--->Epoch: 174 | Loss: 0.6906024813652039--->Epoch: 175 | Loss: 0.690495491027832--->Epoch: 176 | Loss: 0.690389096736908--->Epoch: 177 | Loss: 0.6902831792831421--->Epoch: 178 | Loss: 0.6901779770851135--->Epoch: 179 | Loss: 0.6900733113288879--->Epoch: 180 | Loss: 0.6899691820144653--->Epoch: 181 | Loss: 0.6898656487464905--->Epoch: 182 | Loss: 0.6897627115249634--->Epoch: 183 | Loss: 0.689660370349884--->Epoch: 184 | Loss: 0.6895585060119629--->Epoch: 185 | Loss: 0.6894572377204895--->Epoch: 186 | Loss: 0.6893565058708191--->Epoch: 187 | Loss: 0.6892564296722412--->Epoch: 188 | Loss: 0.6891568303108215--->Epoch: 189 | Loss: 0.6890577673912048--->Epoch: 190 | Loss: 0.6889593005180359--->Epoch: 191 | Loss: 0.6888612508773804--->Epoch: 192 | Loss: 0.6887637972831726--->Epoch: 193 | Loss: 0.6886668801307678--->Epoch: 194 | Loss: 0.6885706186294556--->Epoch: 195 | Loss: 0.6884746551513672--->Epoch: 196 | Loss: 0.6883793473243713--->Epoch: 197 | Loss: 0.6882844567298889--->Epoch: 198 | Loss: 0.688190221786499--->Epoch: 199 | Loss: 0.6880964040756226--->Epoch: 200 | Loss: 0.6880030632019043--->\n",
            "Epoch: 201 | Loss: 0.6879101395606995--->Epoch: 202 | Loss: 0.6878178715705872--->Epoch: 203 | Loss: 0.6877261400222778--->Epoch: 204 | Loss: 0.6876348853111267--->Epoch: 205 | Loss: 0.6875439286231995--->Epoch: 206 | Loss: 0.6874537467956543--->Epoch: 207 | Loss: 0.687363862991333--->Epoch: 208 | Loss: 0.6872743964195251--->Epoch: 209 | Loss: 0.6871855854988098--->Epoch: 210 | Loss: 0.6870971322059631--->Epoch: 211 | Loss: 0.6870090961456299--->Epoch: 212 | Loss: 0.6869217157363892--->Epoch: 213 | Loss: 0.6868346929550171--->Epoch: 214 | Loss: 0.6867481470108032--->Epoch: 215 | Loss: 0.6866620182991028--->Epoch: 216 | Loss: 0.6865763664245605--->Epoch: 217 | Loss: 0.6864911913871765--->Epoch: 218 | Loss: 0.6864064335823059--->Epoch: 219 | Loss: 0.6863221526145935--->Epoch: 220 | Loss: 0.6862383484840393--->Epoch: 221 | Loss: 0.6861549615859985--->Epoch: 222 | Loss: 0.686072051525116--->Epoch: 223 | Loss: 0.6859894394874573--->Epoch: 224 | Loss: 0.6859074234962463--->Epoch: 225 | Loss: 0.685825765132904--->Epoch: 226 | Loss: 0.6857445240020752--->Epoch: 227 | Loss: 0.6856638193130493--->Epoch: 228 | Loss: 0.6855834126472473--->Epoch: 229 | Loss: 0.6855034232139587--->Epoch: 230 | Loss: 0.6854239106178284--->Epoch: 231 | Loss: 0.6853447556495667--->Epoch: 232 | Loss: 0.6852661371231079--->Epoch: 233 | Loss: 0.6851878762245178--->Epoch: 234 | Loss: 0.6851099133491516--->Epoch: 235 | Loss: 0.6850324273109436--->Epoch: 236 | Loss: 0.6849554181098938--->Epoch: 237 | Loss: 0.6848787069320679--->Epoch: 238 | Loss: 0.6848024725914001--->Epoch: 239 | Loss: 0.6847266554832458--->Epoch: 240 | Loss: 0.6846511363983154--->Epoch: 241 | Loss: 0.6845760345458984--->Epoch: 242 | Loss: 0.6845012903213501--->Epoch: 243 | Loss: 0.6844270825386047--->Epoch: 244 | Loss: 0.6843531131744385--->Epoch: 245 | Loss: 0.6842796206474304--->Epoch: 246 | Loss: 0.6842063665390015--->Epoch: 247 | Loss: 0.6841337084770203--->Epoch: 248 | Loss: 0.6840611696243286--->Epoch: 249 | Loss: 0.6839891672134399--->Epoch: 250 | Loss: 0.6839175224304199--->Epoch: 251 | Loss: 0.6838462352752686--->Epoch: 252 | Loss: 0.6837753057479858--->Epoch: 253 | Loss: 0.6837047934532166--->Epoch: 254 | Loss: 0.6836345195770264--->Epoch: 255 | Loss: 0.6835647821426392--->Epoch: 256 | Loss: 0.683495283126831--->Epoch: 257 | Loss: 0.6834261417388916--->Epoch: 258 | Loss: 0.6833573579788208--->Epoch: 259 | Loss: 0.6832889914512634--->Epoch: 260 | Loss: 0.6832209229469299--->Epoch: 261 | Loss: 0.6831531524658203--->Epoch: 262 | Loss: 0.6830857992172241--->Epoch: 263 | Loss: 0.6830188035964966--->Epoch: 264 | Loss: 0.6829521656036377--->Epoch: 265 | Loss: 0.6828858256340027--->Epoch: 266 | Loss: 0.6828198432922363--->Epoch: 267 | Loss: 0.6827540993690491--->Epoch: 268 | Loss: 0.68268883228302--->Epoch: 269 | Loss: 0.6826238632202148--->Epoch: 270 | Loss: 0.6825591921806335--->Epoch: 271 | Loss: 0.6824948191642761--->Epoch: 272 | Loss: 0.6824308633804321--->Epoch: 273 | Loss: 0.6823670864105225--->Epoch: 274 | Loss: 0.682303786277771--->Epoch: 275 | Loss: 0.6822406649589539--->Epoch: 276 | Loss: 0.6821780204772949--->Epoch: 277 | Loss: 0.6821156144142151--->Epoch: 278 | Loss: 0.6820535659790039--->Epoch: 279 | Loss: 0.6819917559623718--->Epoch: 280 | Loss: 0.6819302439689636--->Epoch: 281 | Loss: 0.6818690896034241--->Epoch: 282 | Loss: 0.6818082928657532--->Epoch: 283 | Loss: 0.6817477345466614--->Epoch: 284 | Loss: 0.6816875338554382--->Epoch: 285 | Loss: 0.6816275715827942--->Epoch: 286 | Loss: 0.6815679669380188--->Epoch: 287 | Loss: 0.6815087199211121--->Epoch: 288 | Loss: 0.6814496517181396--->Epoch: 289 | Loss: 0.6813908815383911--->Epoch: 290 | Loss: 0.6813324093818665--->Epoch: 291 | Loss: 0.6812742948532104--->Epoch: 292 | Loss: 0.6812164783477783--->Epoch: 293 | Loss: 0.6811589598655701--->Epoch: 294 | Loss: 0.6811016798019409--->Epoch: 295 | Loss: 0.6810446381568909--->Epoch: 296 | Loss: 0.6809880137443542--->Epoch: 297 | Loss: 0.6809316277503967--->Epoch: 298 | Loss: 0.6808754801750183--->Epoch: 299 | Loss: 0.6808196306228638--->Epoch: 300 | Loss: 0.6807640194892883--->\n",
            "Epoch: 301 | Loss: 0.6807087659835815--->Epoch: 302 | Loss: 0.6806536912918091--->Epoch: 303 | Loss: 0.6805989742279053--->Epoch: 304 | Loss: 0.6805445551872253--->Epoch: 305 | Loss: 0.6804904341697693--->Epoch: 306 | Loss: 0.680436372756958--->Epoch: 307 | Loss: 0.6803827881813049--->Epoch: 308 | Loss: 0.6803293824195862--->Epoch: 309 | Loss: 0.6802763342857361--->Epoch: 310 | Loss: 0.6802233457565308--->Epoch: 311 | Loss: 0.6801707744598389--->Epoch: 312 | Loss: 0.6801184415817261--->Epoch: 313 | Loss: 0.6800663471221924--->Epoch: 314 | Loss: 0.6800144910812378--->Epoch: 315 | Loss: 0.6799628734588623--->Epoch: 316 | Loss: 0.6799116134643555--->Epoch: 317 | Loss: 0.6798605918884277--->Epoch: 318 | Loss: 0.6798098087310791--->Epoch: 319 | Loss: 0.6797592639923096--->Epoch: 320 | Loss: 0.6797088980674744--->Epoch: 321 | Loss: 0.6796589493751526--->Epoch: 322 | Loss: 0.6796090602874756--->Epoch: 323 | Loss: 0.6795595288276672--->Epoch: 324 | Loss: 0.679510235786438--->Epoch: 325 | Loss: 0.6794611215591431--->Epoch: 326 | Loss: 0.679412305355072--->Epoch: 327 | Loss: 0.6793637275695801--->Epoch: 328 | Loss: 0.6793153285980225--->Epoch: 329 | Loss: 0.6792672872543335--->Epoch: 330 | Loss: 0.6792193651199341--->Epoch: 331 | Loss: 0.6791716814041138--->Epoch: 332 | Loss: 0.6791242957115173--->Epoch: 333 | Loss: 0.6790770888328552--->Epoch: 334 | Loss: 0.679030179977417--->Epoch: 335 | Loss: 0.6789835095405579--->Epoch: 336 | Loss: 0.6789370179176331--->Epoch: 337 | Loss: 0.6788907051086426--->Epoch: 338 | Loss: 0.6788446307182312--->Epoch: 339 | Loss: 0.6787989139556885--->Epoch: 340 | Loss: 0.6787533164024353--->Epoch: 341 | Loss: 0.6787078976631165--->Epoch: 342 | Loss: 0.6786627769470215--->Epoch: 343 | Loss: 0.6786178350448608--->Epoch: 344 | Loss: 0.6785731911659241--->Epoch: 345 | Loss: 0.6785287261009216--->Epoch: 346 | Loss: 0.678484320640564--->Epoch: 347 | Loss: 0.6784403920173645--->Epoch: 348 | Loss: 0.678396463394165--->Epoch: 349 | Loss: 0.6783528327941895--->Epoch: 350 | Loss: 0.678309440612793--->Epoch: 351 | Loss: 0.6782662272453308--->Epoch: 352 | Loss: 0.6782232522964478--->Epoch: 353 | Loss: 0.678180456161499--->Epoch: 354 | Loss: 0.6781378984451294--->Epoch: 355 | Loss: 0.6780955791473389--->Epoch: 356 | Loss: 0.6780534386634827--->Epoch: 357 | Loss: 0.678011417388916--->Epoch: 358 | Loss: 0.677969753742218--->Epoch: 359 | Loss: 0.67792809009552--->Epoch: 360 | Loss: 0.6778867244720459--->Epoch: 361 | Loss: 0.6778455972671509--->Epoch: 362 | Loss: 0.6778045892715454--->Epoch: 363 | Loss: 0.6777639389038086--->Epoch: 364 | Loss: 0.677723228931427--->Epoch: 365 | Loss: 0.6776829957962036--->Epoch: 366 | Loss: 0.677642822265625--->Epoch: 367 | Loss: 0.6776028871536255--->Epoch: 368 | Loss: 0.6775630712509155--->Epoch: 369 | Loss: 0.6775234937667847--->Epoch: 370 | Loss: 0.6774840950965881--->Epoch: 371 | Loss: 0.6774448156356812--->Epoch: 372 | Loss: 0.6774058938026428--->Epoch: 373 | Loss: 0.6773669123649597--->Epoch: 374 | Loss: 0.67732834815979--->Epoch: 375 | Loss: 0.6772899031639099--->Epoch: 376 | Loss: 0.6772515177726746--->Epoch: 377 | Loss: 0.6772134900093079--->Epoch: 378 | Loss: 0.6771756410598755--->Epoch: 379 | Loss: 0.6771378517150879--->Epoch: 380 | Loss: 0.6771003007888794--->Epoch: 381 | Loss: 0.67706298828125--->Epoch: 382 | Loss: 0.6770257949829102--->Epoch: 383 | Loss: 0.6769887208938599--->Epoch: 384 | Loss: 0.6769518256187439--->Epoch: 385 | Loss: 0.6769152879714966--->Epoch: 386 | Loss: 0.6768786907196045--->Epoch: 387 | Loss: 0.676842451095581--->Epoch: 388 | Loss: 0.6768063306808472--->Epoch: 389 | Loss: 0.6767703294754028--->Epoch: 390 | Loss: 0.6767345666885376--->Epoch: 391 | Loss: 0.6766989231109619--->Epoch: 392 | Loss: 0.6766634583473206--->Epoch: 393 | Loss: 0.6766281127929688--->Epoch: 394 | Loss: 0.676593005657196--->Epoch: 395 | Loss: 0.6765580773353577--->Epoch: 396 | Loss: 0.6765233874320984--->Epoch: 397 | Loss: 0.6764886379241943--->Epoch: 398 | Loss: 0.6764542460441589--->Epoch: 399 | Loss: 0.6764199733734131--->Epoch: 400 | Loss: 0.676385760307312--->\n",
            "Epoch: 401 | Loss: 0.67635178565979--->Epoch: 402 | Loss: 0.6763179898262024--->Epoch: 403 | Loss: 0.6762843132019043--->Epoch: 404 | Loss: 0.6762509346008301--->Epoch: 405 | Loss: 0.6762175559997559--->Epoch: 406 | Loss: 0.6761844158172607--->Epoch: 407 | Loss: 0.6761513352394104--->Epoch: 408 | Loss: 0.6761185526847839--->Epoch: 409 | Loss: 0.6760857701301575--->Epoch: 410 | Loss: 0.6760532259941101--->Epoch: 411 | Loss: 0.6760208606719971--->Epoch: 412 | Loss: 0.6759885549545288--->Epoch: 413 | Loss: 0.6759564876556396--->Epoch: 414 | Loss: 0.67592453956604--->Epoch: 415 | Loss: 0.67589271068573--->Epoch: 416 | Loss: 0.6758610606193542--->Epoch: 417 | Loss: 0.6758295893669128--->Epoch: 418 | Loss: 0.675798237323761--->Epoch: 419 | Loss: 0.6757671236991882--->Epoch: 420 | Loss: 0.6757359504699707--->Epoch: 421 | Loss: 0.675705075263977--->Epoch: 422 | Loss: 0.6756743788719177--->Epoch: 423 | Loss: 0.6756437420845032--->Epoch: 424 | Loss: 0.6756132245063782--->Epoch: 425 | Loss: 0.6755828857421875--->Epoch: 426 | Loss: 0.6755527853965759--->Epoch: 427 | Loss: 0.6755226850509644--->Epoch: 428 | Loss: 0.6754927039146423--->Epoch: 429 | Loss: 0.6754629611968994--->Epoch: 430 | Loss: 0.6754332780838013--->Epoch: 431 | Loss: 0.6754038333892822--->Epoch: 432 | Loss: 0.6753745079040527--->Epoch: 433 | Loss: 0.675345242023468--->Epoch: 434 | Loss: 0.6753161549568176--->Epoch: 435 | Loss: 0.6752872467041016--->Epoch: 436 | Loss: 0.6752585172653198--->Epoch: 437 | Loss: 0.6752297282218933--->Epoch: 438 | Loss: 0.6752012372016907--->Epoch: 439 | Loss: 0.6751728057861328--->Epoch: 440 | Loss: 0.6751444339752197--->Epoch: 441 | Loss: 0.6751163601875305--->Epoch: 442 | Loss: 0.6750882863998413--->Epoch: 443 | Loss: 0.6750603914260864--->Epoch: 444 | Loss: 0.6750326156616211--->Epoch: 445 | Loss: 0.6750050783157349--->Epoch: 446 | Loss: 0.6749776005744934--->Epoch: 447 | Loss: 0.674950122833252--->Epoch: 448 | Loss: 0.6749229431152344--->Epoch: 449 | Loss: 0.6748958230018616--->Epoch: 450 | Loss: 0.6748687624931335--->Epoch: 451 | Loss: 0.6748419404029846--->Epoch: 452 | Loss: 0.6748152375221252--->Epoch: 453 | Loss: 0.6747885346412659--->Epoch: 454 | Loss: 0.6747620105743408--->Epoch: 455 | Loss: 0.6747356653213501--->Epoch: 456 | Loss: 0.6747094392776489--->Epoch: 457 | Loss: 0.6746832132339478--->Epoch: 458 | Loss: 0.6746572852134705--->Epoch: 459 | Loss: 0.6746313571929932--->Epoch: 460 | Loss: 0.6746055483818054--->Epoch: 461 | Loss: 0.674579918384552--->Epoch: 462 | Loss: 0.6745543479919434--->Epoch: 463 | Loss: 0.674528956413269--->Epoch: 464 | Loss: 0.6745036244392395--->Epoch: 465 | Loss: 0.6744784712791443--->Epoch: 466 | Loss: 0.6744533181190491--->Epoch: 467 | Loss: 0.6744283437728882--->Epoch: 468 | Loss: 0.6744036078453064--->Epoch: 469 | Loss: 0.6743787527084351--->Epoch: 470 | Loss: 0.6743541955947876--->Epoch: 471 | Loss: 0.6743296384811401--->Epoch: 472 | Loss: 0.674305260181427--->Epoch: 473 | Loss: 0.6742810606956482--->Epoch: 474 | Loss: 0.6742568612098694--->Epoch: 475 | Loss: 0.6742327809333801--->Epoch: 476 | Loss: 0.6742088198661804--->Epoch: 477 | Loss: 0.6741849780082703--->Epoch: 478 | Loss: 0.6741612553596497--->Epoch: 479 | Loss: 0.6741376519203186--->Epoch: 480 | Loss: 0.6741139888763428--->Epoch: 481 | Loss: 0.6740906238555908--->Epoch: 482 | Loss: 0.6740673184394836--->Epoch: 483 | Loss: 0.6740441918373108--->Epoch: 484 | Loss: 0.6740211248397827--->Epoch: 485 | Loss: 0.6739980578422546--->Epoch: 486 | Loss: 0.6739751100540161--->Epoch: 487 | Loss: 0.6739524006843567--->Epoch: 488 | Loss: 0.6739296317100525--->Epoch: 489 | Loss: 0.6739071607589722--->Epoch: 490 | Loss: 0.6738846302032471--->Epoch: 491 | Loss: 0.6738622784614563--->Epoch: 492 | Loss: 0.6738400459289551--->Epoch: 493 | Loss: 0.6738178730010986--->Epoch: 494 | Loss: 0.6737958192825317--->Epoch: 495 | Loss: 0.6737737655639648--->Epoch: 496 | Loss: 0.673751950263977--->Epoch: 497 | Loss: 0.6737302541732788--->Epoch: 498 | Loss: 0.6737084984779358--->Epoch: 499 | Loss: 0.6736869215965271--->Epoch: 500 | Loss: 0.673665463924408--->\n",
            "Epoch: 501 | Loss: 0.6736440062522888--->Epoch: 502 | Loss: 0.6736226677894592--->Epoch: 503 | Loss: 0.6736016273498535--->Epoch: 504 | Loss: 0.6735804677009583--->Epoch: 505 | Loss: 0.6735594272613525--->Epoch: 506 | Loss: 0.6735386252403259--->Epoch: 507 | Loss: 0.6735177040100098--->Epoch: 508 | Loss: 0.6734970211982727--->Epoch: 509 | Loss: 0.6734763979911804--->Epoch: 510 | Loss: 0.6734558939933777--->Epoch: 511 | Loss: 0.6734354496002197--->Epoch: 512 | Loss: 0.6734150648117065--->Epoch: 513 | Loss: 0.6733947992324829--->Epoch: 514 | Loss: 0.6733747720718384--->Epoch: 515 | Loss: 0.6733545064926147--->Epoch: 516 | Loss: 0.673334538936615--->Epoch: 517 | Loss: 0.6733146905899048--->Epoch: 518 | Loss: 0.6732948422431946--->Epoch: 519 | Loss: 0.6732751727104187--->Epoch: 520 | Loss: 0.6732555031776428--->Epoch: 521 | Loss: 0.6732359528541565--->Epoch: 522 | Loss: 0.6732165217399597--->Epoch: 523 | Loss: 0.6731970906257629--->Epoch: 524 | Loss: 0.6731778979301453--->Epoch: 525 | Loss: 0.6731586456298828--->Epoch: 526 | Loss: 0.6731395721435547--->Epoch: 527 | Loss: 0.673120379447937--->Epoch: 528 | Loss: 0.673101544380188--->Epoch: 529 | Loss: 0.673082709312439--->Epoch: 530 | Loss: 0.6730638742446899--->Epoch: 531 | Loss: 0.6730452179908752--->Epoch: 532 | Loss: 0.6730265617370605--->Epoch: 533 | Loss: 0.6730080842971802--->Epoch: 534 | Loss: 0.6729896068572998--->Epoch: 535 | Loss: 0.672971248626709--->Epoch: 536 | Loss: 0.6729528903961182--->Epoch: 537 | Loss: 0.6729347109794617--->Epoch: 538 | Loss: 0.67291659116745--->Epoch: 539 | Loss: 0.6728985905647278--->Epoch: 540 | Loss: 0.6728805303573608--->Epoch: 541 | Loss: 0.672862708568573--->Epoch: 542 | Loss: 0.6728448867797852--->Epoch: 543 | Loss: 0.6728271842002869--->Epoch: 544 | Loss: 0.6728096008300781--->Epoch: 545 | Loss: 0.6727919578552246--->Epoch: 546 | Loss: 0.6727744340896606--->Epoch: 547 | Loss: 0.672757089138031--->Epoch: 548 | Loss: 0.6727396845817566--->Epoch: 549 | Loss: 0.6727225184440613--->Epoch: 550 | Loss: 0.6727052330970764--->Epoch: 551 | Loss: 0.6726881861686707--->Epoch: 552 | Loss: 0.6726710796356201--->Epoch: 553 | Loss: 0.6726540923118591--->Epoch: 554 | Loss: 0.6726372838020325--->Epoch: 555 | Loss: 0.6726204752922058--->Epoch: 556 | Loss: 0.6726037263870239--->Epoch: 557 | Loss: 0.6725870370864868--->Epoch: 558 | Loss: 0.6725704073905945--->Epoch: 559 | Loss: 0.6725538969039917--->Epoch: 560 | Loss: 0.6725374460220337--->Epoch: 561 | Loss: 0.6725211143493652--->Epoch: 562 | Loss: 0.6725047826766968--->Epoch: 563 | Loss: 0.6724885106086731--->Epoch: 564 | Loss: 0.672472357749939--->Epoch: 565 | Loss: 0.6724563241004944--->Epoch: 566 | Loss: 0.672440230846405--->Epoch: 567 | Loss: 0.67242431640625--->Epoch: 568 | Loss: 0.672408401966095--->Epoch: 569 | Loss: 0.6723926067352295--->Epoch: 570 | Loss: 0.672376811504364--->Epoch: 571 | Loss: 0.6723611354827881--->Epoch: 572 | Loss: 0.6723455190658569--->Epoch: 573 | Loss: 0.6723300218582153--->Epoch: 574 | Loss: 0.672314465045929--->Epoch: 575 | Loss: 0.6722990870475769--->Epoch: 576 | Loss: 0.6722837686538696--->Epoch: 577 | Loss: 0.6722685098648071--->Epoch: 578 | Loss: 0.6722532510757446--->Epoch: 579 | Loss: 0.6722381114959717--->Epoch: 580 | Loss: 0.6722230315208435--->Epoch: 581 | Loss: 0.6722080707550049--->Epoch: 582 | Loss: 0.6721930503845215--->Epoch: 583 | Loss: 0.6721781492233276--->Epoch: 584 | Loss: 0.6721633672714233--->Epoch: 585 | Loss: 0.672148585319519--->Epoch: 586 | Loss: 0.6721339225769043--->Epoch: 587 | Loss: 0.6721193194389343--->Epoch: 588 | Loss: 0.6721047163009644--->Epoch: 589 | Loss: 0.6720901727676392--->Epoch: 590 | Loss: 0.6720758080482483--->Epoch: 591 | Loss: 0.6720613241195679--->Epoch: 592 | Loss: 0.6720470786094666--->Epoch: 593 | Loss: 0.6720328330993652--->Epoch: 594 | Loss: 0.6720186471939087--->Epoch: 595 | Loss: 0.6720044612884521--->Epoch: 596 | Loss: 0.6719903349876404--->Epoch: 597 | Loss: 0.6719763875007629--->Epoch: 598 | Loss: 0.6719624996185303--->Epoch: 599 | Loss: 0.6719484925270081--->Epoch: 600 | Loss: 0.6719347238540649--->\n",
            "Epoch: 601 | Loss: 0.671920895576477--->Epoch: 602 | Loss: 0.6719072461128235--->Epoch: 603 | Loss: 0.6718935370445251--->Epoch: 604 | Loss: 0.6718798875808716--->Epoch: 605 | Loss: 0.6718664169311523--->Epoch: 606 | Loss: 0.6718528866767883--->Epoch: 607 | Loss: 0.6718394756317139--->Epoch: 608 | Loss: 0.6718260049819946--->Epoch: 609 | Loss: 0.6718128323554993--->Epoch: 610 | Loss: 0.6717995405197144--->Epoch: 611 | Loss: 0.6717863082885742--->Epoch: 612 | Loss: 0.6717731952667236--->Epoch: 613 | Loss: 0.671760082244873--->Epoch: 614 | Loss: 0.671747088432312--->Epoch: 615 | Loss: 0.671734094619751--->Epoch: 616 | Loss: 0.6717211008071899--->Epoch: 617 | Loss: 0.6717082858085632--->Epoch: 618 | Loss: 0.6716954708099365--->Epoch: 619 | Loss: 0.6716827154159546--->Epoch: 620 | Loss: 0.6716699600219727--->Epoch: 621 | Loss: 0.6716573238372803--->Epoch: 622 | Loss: 0.6716447472572327--->Epoch: 623 | Loss: 0.6716322898864746--->Epoch: 624 | Loss: 0.6716196537017822--->Epoch: 625 | Loss: 0.6716071963310242--->Epoch: 626 | Loss: 0.6715948581695557--->Epoch: 627 | Loss: 0.6715825200080872--->Epoch: 628 | Loss: 0.6715702414512634--->Epoch: 629 | Loss: 0.6715580224990845--->Epoch: 630 | Loss: 0.6715458631515503--->Epoch: 631 | Loss: 0.6715336441993713--->Epoch: 632 | Loss: 0.6715216040611267--->Epoch: 633 | Loss: 0.6715096235275269--->Epoch: 634 | Loss: 0.6714975833892822--->Epoch: 635 | Loss: 0.6714856028556824--->Epoch: 636 | Loss: 0.6714738011360168--->Epoch: 637 | Loss: 0.6714619994163513--->Epoch: 638 | Loss: 0.671450138092041--->Epoch: 639 | Loss: 0.671438455581665--->Epoch: 640 | Loss: 0.6714268326759338--->Epoch: 641 | Loss: 0.6714151501655579--->Epoch: 642 | Loss: 0.6714035868644714--->Epoch: 643 | Loss: 0.6713919639587402--->Epoch: 644 | Loss: 0.6713805198669434--->Epoch: 645 | Loss: 0.6713690757751465--->Epoch: 646 | Loss: 0.6713576912879944--->Epoch: 647 | Loss: 0.6713463068008423--->Epoch: 648 | Loss: 0.6713350415229797--->Epoch: 649 | Loss: 0.671323835849762--->Epoch: 650 | Loss: 0.6713125705718994--->Epoch: 651 | Loss: 0.6713014245033264--->Epoch: 652 | Loss: 0.671290397644043--->Epoch: 653 | Loss: 0.6712793111801147--->Epoch: 654 | Loss: 0.6712682843208313--->Epoch: 655 | Loss: 0.6712573170661926--->Epoch: 656 | Loss: 0.671246349811554--->Epoch: 657 | Loss: 0.6712355017662048--->Epoch: 658 | Loss: 0.6712246537208557--->Epoch: 659 | Loss: 0.6712138652801514--->Epoch: 660 | Loss: 0.671203076839447--->Epoch: 661 | Loss: 0.6711923480033875--->Epoch: 662 | Loss: 0.6711816787719727--->Epoch: 663 | Loss: 0.6711711287498474--->Epoch: 664 | Loss: 0.6711604595184326--->Epoch: 665 | Loss: 0.6711499691009521--->Epoch: 666 | Loss: 0.6711394786834717--->Epoch: 667 | Loss: 0.6711291074752808--->Epoch: 668 | Loss: 0.6711186766624451--->Epoch: 669 | Loss: 0.6711084246635437--->Epoch: 670 | Loss: 0.671097993850708--->Epoch: 671 | Loss: 0.6710877418518066--->Epoch: 672 | Loss: 0.67107754945755--->Epoch: 673 | Loss: 0.6710672974586487--->Epoch: 674 | Loss: 0.6710571050643921--->Epoch: 675 | Loss: 0.671047031879425--->Epoch: 676 | Loss: 0.6710370182991028--->Epoch: 677 | Loss: 0.671026885509491--->Epoch: 678 | Loss: 0.6710169911384583--->Epoch: 679 | Loss: 0.6710070371627808--->Epoch: 680 | Loss: 0.6709970831871033--->Epoch: 681 | Loss: 0.6709872484207153--->Epoch: 682 | Loss: 0.6709774732589722--->Epoch: 683 | Loss: 0.6709676384925842--->Epoch: 684 | Loss: 0.6709579229354858--->Epoch: 685 | Loss: 0.670948326587677--->Epoch: 686 | Loss: 0.6709384918212891--->Epoch: 687 | Loss: 0.670928955078125--->Epoch: 688 | Loss: 0.6709193587303162--->Epoch: 689 | Loss: 0.6709098219871521--->Epoch: 690 | Loss: 0.6709004044532776--->Epoch: 691 | Loss: 0.6708908081054688--->Epoch: 692 | Loss: 0.6708813309669495--->Epoch: 693 | Loss: 0.6708720326423645--->Epoch: 694 | Loss: 0.67086261510849--->Epoch: 695 | Loss: 0.6708533763885498--->Epoch: 696 | Loss: 0.6708440780639648--->Epoch: 697 | Loss: 0.6708348393440247--->Epoch: 698 | Loss: 0.6708256006240845--->Epoch: 699 | Loss: 0.6708164811134338--->Epoch: 700 | Loss: 0.670807421207428--->\n",
            "Epoch: 701 | Loss: 0.6707982420921326--->Epoch: 702 | Loss: 0.6707891821861267--->Epoch: 703 | Loss: 0.6707801818847656--->Epoch: 704 | Loss: 0.6707711815834045--->Epoch: 705 | Loss: 0.6707622408866882--->Epoch: 706 | Loss: 0.6707533597946167--->Epoch: 707 | Loss: 0.6707444787025452--->Epoch: 708 | Loss: 0.6707355976104736--->Epoch: 709 | Loss: 0.6707268357276917--->Epoch: 710 | Loss: 0.6707179546356201--->Epoch: 711 | Loss: 0.6707093119621277--->Epoch: 712 | Loss: 0.6707006096839905--->Epoch: 713 | Loss: 0.670691967010498--->Epoch: 714 | Loss: 0.6706832647323608--->Epoch: 715 | Loss: 0.670674741268158--->Epoch: 716 | Loss: 0.6706661581993103--->Epoch: 717 | Loss: 0.6706575751304626--->Epoch: 718 | Loss: 0.6706490516662598--->Epoch: 719 | Loss: 0.6706406474113464--->Epoch: 720 | Loss: 0.6706322431564331--->Epoch: 721 | Loss: 0.6706238389015198--->Epoch: 722 | Loss: 0.6706154942512512--->Epoch: 723 | Loss: 0.6706072092056274--->Epoch: 724 | Loss: 0.6705988049507141--->Epoch: 725 | Loss: 0.6705906391143799--->Epoch: 726 | Loss: 0.6705823540687561--->Epoch: 727 | Loss: 0.6705741286277771--->Epoch: 728 | Loss: 0.6705660820007324--->Epoch: 729 | Loss: 0.6705578565597534--->Epoch: 730 | Loss: 0.6705498099327087--->Epoch: 731 | Loss: 0.6705417633056641--->Epoch: 732 | Loss: 0.6705336570739746--->Epoch: 733 | Loss: 0.6705257296562195--->Epoch: 734 | Loss: 0.6705177426338196--->Epoch: 735 | Loss: 0.6705097556114197--->Epoch: 736 | Loss: 0.6705018877983093--->Epoch: 737 | Loss: 0.6704939603805542--->Epoch: 738 | Loss: 0.6704862117767334--->Epoch: 739 | Loss: 0.670478343963623--->Epoch: 740 | Loss: 0.6704705953598022--->Epoch: 741 | Loss: 0.6704627871513367--->Epoch: 742 | Loss: 0.6704550385475159--->Epoch: 743 | Loss: 0.6704474091529846--->Epoch: 744 | Loss: 0.6704397201538086--->Epoch: 745 | Loss: 0.6704320907592773--->Epoch: 746 | Loss: 0.6704245209693909--->Epoch: 747 | Loss: 0.6704168915748596--->Epoch: 748 | Loss: 0.6704093813896179--->Epoch: 749 | Loss: 0.6704018115997314--->Epoch: 750 | Loss: 0.6703943610191345--->Epoch: 751 | Loss: 0.6703869700431824--->Epoch: 752 | Loss: 0.6703794598579407--->Epoch: 753 | Loss: 0.6703721880912781--->Epoch: 754 | Loss: 0.6703646779060364--->Epoch: 755 | Loss: 0.6703574657440186--->Epoch: 756 | Loss: 0.6703500747680664--->Epoch: 757 | Loss: 0.6703428030014038--->Epoch: 758 | Loss: 0.6703354716300964--->Epoch: 759 | Loss: 0.6703283786773682--->Epoch: 760 | Loss: 0.6703210473060608--->Epoch: 761 | Loss: 0.6703138947486877--->Epoch: 762 | Loss: 0.6703068017959595--->Epoch: 763 | Loss: 0.6702996492385864--->Epoch: 764 | Loss: 0.6702926754951477--->Epoch: 765 | Loss: 0.6702855229377747--->Epoch: 766 | Loss: 0.6702784299850464--->Epoch: 767 | Loss: 0.6702714562416077--->Epoch: 768 | Loss: 0.670264482498169--->Epoch: 769 | Loss: 0.6702575087547302--->Epoch: 770 | Loss: 0.670250654220581--->Epoch: 771 | Loss: 0.6702437996864319--->Epoch: 772 | Loss: 0.6702368259429932--->Epoch: 773 | Loss: 0.670229971408844--->Epoch: 774 | Loss: 0.6702231764793396--->Epoch: 775 | Loss: 0.6702163815498352--->Epoch: 776 | Loss: 0.6702096462249756--->Epoch: 777 | Loss: 0.6702028512954712--->Epoch: 778 | Loss: 0.6701961755752563--->Epoch: 779 | Loss: 0.6701894402503967--->Epoch: 780 | Loss: 0.6701828241348267--->Epoch: 781 | Loss: 0.6701761484146118--->Epoch: 782 | Loss: 0.6701695322990417--->Epoch: 783 | Loss: 0.6701629757881165--->Epoch: 784 | Loss: 0.6701563596725464--->Epoch: 785 | Loss: 0.6701498627662659--->Epoch: 786 | Loss: 0.6701434254646301--->Epoch: 787 | Loss: 0.6701369285583496--->Epoch: 788 | Loss: 0.6701303720474243--->Epoch: 789 | Loss: 0.6701239347457886--->Epoch: 790 | Loss: 0.6701174974441528--->Epoch: 791 | Loss: 0.6701111793518066--->Epoch: 792 | Loss: 0.6701047420501709--->Epoch: 793 | Loss: 0.6700983643531799--->Epoch: 794 | Loss: 0.6700921058654785--->Epoch: 795 | Loss: 0.6700857281684875--->Epoch: 796 | Loss: 0.6700795292854309--->Epoch: 797 | Loss: 0.6700732707977295--->Epoch: 798 | Loss: 0.6700670123100281--->Epoch: 799 | Loss: 0.6700608134269714--->Epoch: 800 | Loss: 0.6700546145439148--->\n",
            "Epoch: 801 | Loss: 0.6700484752655029--->Epoch: 802 | Loss: 0.6700423955917358--->Epoch: 803 | Loss: 0.670036256313324--->Epoch: 804 | Loss: 0.6700301766395569--->Epoch: 805 | Loss: 0.6700241565704346--->Epoch: 806 | Loss: 0.6700181365013123--->Epoch: 807 | Loss: 0.6700120568275452--->Epoch: 808 | Loss: 0.6700060963630676--->Epoch: 809 | Loss: 0.6700001358985901--->Epoch: 810 | Loss: 0.6699941754341125--->Epoch: 811 | Loss: 0.669988214969635--->Epoch: 812 | Loss: 0.6699823141098022--->Epoch: 813 | Loss: 0.6699763536453247--->Epoch: 814 | Loss: 0.6699705719947815--->Epoch: 815 | Loss: 0.6699646711349487--->Epoch: 816 | Loss: 0.6699589490890503--->Epoch: 817 | Loss: 0.6699531078338623--->Epoch: 818 | Loss: 0.6699473261833191--->Epoch: 819 | Loss: 0.6699416637420654--->Epoch: 820 | Loss: 0.6699357628822327--->Epoch: 821 | Loss: 0.669930100440979--->Epoch: 822 | Loss: 0.6699244379997253--->Epoch: 823 | Loss: 0.6699187755584717--->Epoch: 824 | Loss: 0.669913113117218--->Epoch: 825 | Loss: 0.6699075698852539--->Epoch: 826 | Loss: 0.669901967048645--->Epoch: 827 | Loss: 0.6698963642120361--->Epoch: 828 | Loss: 0.6698907017707825--->Epoch: 829 | Loss: 0.6698852181434631--->Epoch: 830 | Loss: 0.669879674911499--->Epoch: 831 | Loss: 0.6698741912841797--->Epoch: 832 | Loss: 0.6698686480522156--->Epoch: 833 | Loss: 0.669863224029541--->Epoch: 834 | Loss: 0.6698578000068665--->Epoch: 835 | Loss: 0.6698523759841919--->Epoch: 836 | Loss: 0.6698469519615173--->Epoch: 837 | Loss: 0.6698415279388428--->Epoch: 838 | Loss: 0.669836163520813--->Epoch: 839 | Loss: 0.6698309183120728--->Epoch: 840 | Loss: 0.669825553894043--->Epoch: 841 | Loss: 0.669820249080658--->Epoch: 842 | Loss: 0.669814944267273--->Epoch: 843 | Loss: 0.6698096394538879--->Epoch: 844 | Loss: 0.6698044538497925--->Epoch: 845 | Loss: 0.6697991490364075--->Epoch: 846 | Loss: 0.6697940230369568--->Epoch: 847 | Loss: 0.6697888374328613--->Epoch: 848 | Loss: 0.6697835922241211--->Epoch: 849 | Loss: 0.6697784662246704--->Epoch: 850 | Loss: 0.6697733402252197--->Epoch: 851 | Loss: 0.669768214225769--->Epoch: 852 | Loss: 0.6697631478309631--->Epoch: 853 | Loss: 0.6697580218315125--->Epoch: 854 | Loss: 0.6697529554367065--->Epoch: 855 | Loss: 0.6697478890419006--->Epoch: 856 | Loss: 0.6697429418563843--->Epoch: 857 | Loss: 0.6697379350662231--->Epoch: 858 | Loss: 0.6697328686714172--->Epoch: 859 | Loss: 0.6697279810905457--->Epoch: 860 | Loss: 0.6697230935096741--->Epoch: 861 | Loss: 0.6697180867195129--->Epoch: 862 | Loss: 0.6697131991386414--->Epoch: 863 | Loss: 0.6697081923484802--->Epoch: 864 | Loss: 0.6697033643722534--->Epoch: 865 | Loss: 0.6696985363960266--->Epoch: 866 | Loss: 0.669693648815155--->Epoch: 867 | Loss: 0.6696888208389282--->Epoch: 868 | Loss: 0.6696839928627014--->Epoch: 869 | Loss: 0.6696792244911194--->Epoch: 870 | Loss: 0.6696744561195374--->Epoch: 871 | Loss: 0.6696697473526001--->Epoch: 872 | Loss: 0.6696649789810181--->Epoch: 873 | Loss: 0.669660210609436--->Epoch: 874 | Loss: 0.6696555614471436--->Epoch: 875 | Loss: 0.6696508526802063--->Epoch: 876 | Loss: 0.6696460843086243--->Epoch: 877 | Loss: 0.6696414947509766--->Epoch: 878 | Loss: 0.6696369051933289--->Epoch: 879 | Loss: 0.6696321964263916--->Epoch: 880 | Loss: 0.6696276068687439--->Epoch: 881 | Loss: 0.669623076915741--->Epoch: 882 | Loss: 0.6696184277534485--->Epoch: 883 | Loss: 0.6696138978004456--->Epoch: 884 | Loss: 0.6696093678474426--->Epoch: 885 | Loss: 0.6696048378944397--->Epoch: 886 | Loss: 0.6696003079414368--->Epoch: 887 | Loss: 0.6695957779884338--->Epoch: 888 | Loss: 0.6695913076400757--->Epoch: 889 | Loss: 0.6695868372917175--->Epoch: 890 | Loss: 0.6695823073387146--->Epoch: 891 | Loss: 0.669577956199646--->Epoch: 892 | Loss: 0.6695735454559326--->Epoch: 893 | Loss: 0.6695691347122192--->Epoch: 894 | Loss: 0.6695647239685059--->Epoch: 895 | Loss: 0.6695603728294373--->Epoch: 896 | Loss: 0.6695560812950134--->Epoch: 897 | Loss: 0.6695516705513--->Epoch: 898 | Loss: 0.6695473194122314--->Epoch: 899 | Loss: 0.6695430874824524--->Epoch: 900 | Loss: 0.6695387959480286--->\n",
            "Epoch: 901 | Loss: 0.6695345044136047--->Epoch: 902 | Loss: 0.6695300936698914--->Epoch: 903 | Loss: 0.6695259213447571--->Epoch: 904 | Loss: 0.6695216298103333--->Epoch: 905 | Loss: 0.6695175170898438--->Epoch: 906 | Loss: 0.6695132851600647--->Epoch: 907 | Loss: 0.6695090532302856--->Epoch: 908 | Loss: 0.6695048809051514--->Epoch: 909 | Loss: 0.6695007681846619--->Epoch: 910 | Loss: 0.6694965958595276--->Epoch: 911 | Loss: 0.6694924235343933--->Epoch: 912 | Loss: 0.6694883704185486--->Epoch: 913 | Loss: 0.6694841980934143--->Epoch: 914 | Loss: 0.6694800853729248--->Epoch: 915 | Loss: 0.6694760322570801--->Epoch: 916 | Loss: 0.6694719791412354--->Epoch: 917 | Loss: 0.6694679260253906--->Epoch: 918 | Loss: 0.6694639325141907--->Epoch: 919 | Loss: 0.6694598197937012--->Epoch: 920 | Loss: 0.6694558262825012--->Epoch: 921 | Loss: 0.669451892375946--->Epoch: 922 | Loss: 0.6694478392601013--->Epoch: 923 | Loss: 0.6694439053535461--->Epoch: 924 | Loss: 0.669439971446991--->Epoch: 925 | Loss: 0.669435977935791--->Epoch: 926 | Loss: 0.6694320440292358--->Epoch: 927 | Loss: 0.6694282293319702--->Epoch: 928 | Loss: 0.669424295425415--->Epoch: 929 | Loss: 0.6694203615188599--->Epoch: 930 | Loss: 0.6694164872169495--->Epoch: 931 | Loss: 0.6694126725196838--->Epoch: 932 | Loss: 0.6694087982177734--->Epoch: 933 | Loss: 0.6694049835205078--->Epoch: 934 | Loss: 0.6694011092185974--->Epoch: 935 | Loss: 0.6693972945213318--->Epoch: 936 | Loss: 0.6693934798240662--->Epoch: 937 | Loss: 0.6693897247314453--->Epoch: 938 | Loss: 0.6693859696388245--->Epoch: 939 | Loss: 0.6693822741508484--->Epoch: 940 | Loss: 0.6693785190582275--->Epoch: 941 | Loss: 0.6693747639656067--->Epoch: 942 | Loss: 0.6693710684776306--->Epoch: 943 | Loss: 0.6693673133850098--->Epoch: 944 | Loss: 0.6693636178970337--->Epoch: 945 | Loss: 0.6693599224090576--->Epoch: 946 | Loss: 0.6693562269210815--->Epoch: 947 | Loss: 0.6693525910377502--->Epoch: 948 | Loss: 0.669348955154419--->Epoch: 949 | Loss: 0.6693453192710876--->Epoch: 950 | Loss: 0.6693416237831116--->Epoch: 951 | Loss: 0.6693381071090698--->Epoch: 952 | Loss: 0.6693344712257385--->Epoch: 953 | Loss: 0.669330894947052--->Epoch: 954 | Loss: 0.6693273186683655--->Epoch: 955 | Loss: 0.669323742389679--->Epoch: 956 | Loss: 0.6693202257156372--->Epoch: 957 | Loss: 0.6693166494369507--->Epoch: 958 | Loss: 0.6693131327629089--->Epoch: 959 | Loss: 0.6693096160888672--->Epoch: 960 | Loss: 0.6693061590194702--->Epoch: 961 | Loss: 0.6693027019500732--->Epoch: 962 | Loss: 0.6692991256713867--->Epoch: 963 | Loss: 0.6692957282066345--->Epoch: 964 | Loss: 0.6692922115325928--->Epoch: 965 | Loss: 0.6692888140678406--->Epoch: 966 | Loss: 0.6692853569984436--->Epoch: 967 | Loss: 0.6692819595336914--->Epoch: 968 | Loss: 0.6692784428596497--->Epoch: 969 | Loss: 0.669275164604187--->Epoch: 970 | Loss: 0.6692717671394348--->Epoch: 971 | Loss: 0.6692683696746826--->Epoch: 972 | Loss: 0.6692650318145752--->Epoch: 973 | Loss: 0.669261634349823--->Epoch: 974 | Loss: 0.6692582368850708--->Epoch: 975 | Loss: 0.6692548990249634--->Epoch: 976 | Loss: 0.6692516207695007--->Epoch: 977 | Loss: 0.6692482829093933--->Epoch: 978 | Loss: 0.6692449450492859--->Epoch: 979 | Loss: 0.6692417860031128--->Epoch: 980 | Loss: 0.6692384481430054--->Epoch: 981 | Loss: 0.6692352294921875--->Epoch: 982 | Loss: 0.6692319512367249--->Epoch: 983 | Loss: 0.6692286729812622--->Epoch: 984 | Loss: 0.6692253947257996--->Epoch: 985 | Loss: 0.6692221760749817--->Epoch: 986 | Loss: 0.669218897819519--->Epoch: 987 | Loss: 0.669215738773346--->Epoch: 988 | Loss: 0.6692125797271729--->Epoch: 989 | Loss: 0.669209361076355--->Epoch: 990 | Loss: 0.6692062616348267--->Epoch: 991 | Loss: 0.6692030429840088--->Epoch: 992 | Loss: 0.6691998243331909--->Epoch: 993 | Loss: 0.6691967248916626--->Epoch: 994 | Loss: 0.6691936254501343--->Epoch: 995 | Loss: 0.6691904664039612--->Epoch: 996 | Loss: 0.6691873669624329--->Epoch: 997 | Loss: 0.6691842079162598--->Epoch: 998 | Loss: 0.6691811084747314--->Epoch: 999 | Loss: 0.6691780686378479--->Epoch: 1000 | Loss: 0.6691749691963196--->\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[ 0.1140, -0.0386, -0.1045,  0.1576, -0.0161, -0.0900,  0.0852,  0.0739,\n",
              "           0.0320, -0.0567, -0.0635, -0.0026, -0.0344,  0.0437,  0.0054,  0.1374,\n",
              "          -0.0676,  0.0683, -0.0709, -0.0045, -0.0544,  0.0536, -0.1299,  0.0648,\n",
              "           0.0231, -0.1236, -0.0851, -0.0961,  0.0832,  0.1481]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.4111], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_pred = model(X_test_tensor)\n",
        "  y_pred = (y_pred > 0.5).float()\n",
        "  accuracy = (y_pred == Y_test_tensor).float().mean()\n",
        "  print(f'Accuracy: {accuracy.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CG8CFfsmLOw",
        "outputId": "1cee0676-3c58-437e-f821-6e394d0486a8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6842105388641357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using built loss function and optim from nn.optim class"
      ],
      "metadata": {
        "id": "ZlC-rBrioj98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNN(nn.Module):\n",
        "  def __init__(self, num_features):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(num_features, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.linear(x)\n",
        "    return self.sigmoid(z)"
      ],
      "metadata": {
        "id": "kvK_FS8hmLLl"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = nn.BCELoss()"
      ],
      "metadata": {
        "id": "79abXoZkmLIs"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "MzZ_VE0kp-lH"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyNN(X_train_tensor.shape[1])\n",
        "\n",
        "for i in range(1000):\n",
        "  # forward pass\n",
        "  y_pred = model(X_train_tensor)\n",
        "\n",
        "  # loss calculation\n",
        "  loss = loss_function(y_pred, Y_train_tensor.reshape(-1, 1))\n",
        "  print(f'Epoch: {i + 1} | Loss: {loss.item()}', end='--->')\n",
        "  if (i + 1) % 100 == 0:\n",
        "    print()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  '''\n",
        "    # weight update\n",
        "    with torch.no_grad():\n",
        "      model.linear.weight -= lr * model.linear.weight.grad\n",
        "      model.linear.bias -= lr * model.linear.bias.grad\n",
        "\n",
        "    # zero the gradients\n",
        "    model.linear.weight.grad.zero_()\n",
        "    model.linear.bias.grad.zero_()\n",
        "  '''\n",
        "  optimizer.step()\n",
        "\n",
        "model.linear.weight, model.linear.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELdPPlJjo_hq",
        "outputId": "3e07c1ef-a3ad-4df1-a466-bc84e44052eb"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | Loss: 0.5073736310005188--->Epoch: 2 | Loss: 0.5073736310005188--->Epoch: 3 | Loss: 0.5073736310005188--->Epoch: 4 | Loss: 0.5073736310005188--->Epoch: 5 | Loss: 0.5073736310005188--->Epoch: 6 | Loss: 0.5073736310005188--->Epoch: 7 | Loss: 0.5073736310005188--->Epoch: 8 | Loss: 0.5073736310005188--->Epoch: 9 | Loss: 0.5073736310005188--->Epoch: 10 | Loss: 0.5073736310005188--->Epoch: 11 | Loss: 0.5073736310005188--->Epoch: 12 | Loss: 0.5073736310005188--->Epoch: 13 | Loss: 0.5073736310005188--->Epoch: 14 | Loss: 0.5073736310005188--->Epoch: 15 | Loss: 0.5073736310005188--->Epoch: 16 | Loss: 0.5073736310005188--->Epoch: 17 | Loss: 0.5073736310005188--->Epoch: 18 | Loss: 0.5073736310005188--->Epoch: 19 | Loss: 0.5073736310005188--->Epoch: 20 | Loss: 0.5073736310005188--->Epoch: 21 | Loss: 0.5073736310005188--->Epoch: 22 | Loss: 0.5073736310005188--->Epoch: 23 | Loss: 0.5073736310005188--->Epoch: 24 | Loss: 0.5073736310005188--->Epoch: 25 | Loss: 0.5073736310005188--->Epoch: 26 | Loss: 0.5073736310005188--->Epoch: 27 | Loss: 0.5073736310005188--->Epoch: 28 | Loss: 0.5073736310005188--->Epoch: 29 | Loss: 0.5073736310005188--->Epoch: 30 | Loss: 0.5073736310005188--->Epoch: 31 | Loss: 0.5073736310005188--->Epoch: 32 | Loss: 0.5073736310005188--->Epoch: 33 | Loss: 0.5073736310005188--->Epoch: 34 | Loss: 0.5073736310005188--->Epoch: 35 | Loss: 0.5073736310005188--->Epoch: 36 | Loss: 0.5073736310005188--->Epoch: 37 | Loss: 0.5073736310005188--->Epoch: 38 | Loss: 0.5073736310005188--->Epoch: 39 | Loss: 0.5073736310005188--->Epoch: 40 | Loss: 0.5073736310005188--->Epoch: 41 | Loss: 0.5073736310005188--->Epoch: 42 | Loss: 0.5073736310005188--->Epoch: 43 | Loss: 0.5073736310005188--->Epoch: 44 | Loss: 0.5073736310005188--->Epoch: 45 | Loss: 0.5073736310005188--->Epoch: 46 | Loss: 0.5073736310005188--->Epoch: 47 | Loss: 0.5073736310005188--->Epoch: 48 | Loss: 0.5073736310005188--->Epoch: 49 | Loss: 0.5073736310005188--->Epoch: 50 | Loss: 0.5073736310005188--->Epoch: 51 | Loss: 0.5073736310005188--->Epoch: 52 | Loss: 0.5073736310005188--->Epoch: 53 | Loss: 0.5073736310005188--->Epoch: 54 | Loss: 0.5073736310005188--->Epoch: 55 | Loss: 0.5073736310005188--->Epoch: 56 | Loss: 0.5073736310005188--->Epoch: 57 | Loss: 0.5073736310005188--->Epoch: 58 | Loss: 0.5073736310005188--->Epoch: 59 | Loss: 0.5073736310005188--->Epoch: 60 | Loss: 0.5073736310005188--->Epoch: 61 | Loss: 0.5073736310005188--->Epoch: 62 | Loss: 0.5073736310005188--->Epoch: 63 | Loss: 0.5073736310005188--->Epoch: 64 | Loss: 0.5073736310005188--->Epoch: 65 | Loss: 0.5073736310005188--->Epoch: 66 | Loss: 0.5073736310005188--->Epoch: 67 | Loss: 0.5073736310005188--->Epoch: 68 | Loss: 0.5073736310005188--->Epoch: 69 | Loss: 0.5073736310005188--->Epoch: 70 | Loss: 0.5073736310005188--->Epoch: 71 | Loss: 0.5073736310005188--->Epoch: 72 | Loss: 0.5073736310005188--->Epoch: 73 | Loss: 0.5073736310005188--->Epoch: 74 | Loss: 0.5073736310005188--->Epoch: 75 | Loss: 0.5073736310005188--->Epoch: 76 | Loss: 0.5073736310005188--->Epoch: 77 | Loss: 0.5073736310005188--->Epoch: 78 | Loss: 0.5073736310005188--->Epoch: 79 | Loss: 0.5073736310005188--->Epoch: 80 | Loss: 0.5073736310005188--->Epoch: 81 | Loss: 0.5073736310005188--->Epoch: 82 | Loss: 0.5073736310005188--->Epoch: 83 | Loss: 0.5073736310005188--->Epoch: 84 | Loss: 0.5073736310005188--->Epoch: 85 | Loss: 0.5073736310005188--->Epoch: 86 | Loss: 0.5073736310005188--->Epoch: 87 | Loss: 0.5073736310005188--->Epoch: 88 | Loss: 0.5073736310005188--->Epoch: 89 | Loss: 0.5073736310005188--->Epoch: 90 | Loss: 0.5073736310005188--->Epoch: 91 | Loss: 0.5073736310005188--->Epoch: 92 | Loss: 0.5073736310005188--->Epoch: 93 | Loss: 0.5073736310005188--->Epoch: 94 | Loss: 0.5073736310005188--->Epoch: 95 | Loss: 0.5073736310005188--->Epoch: 96 | Loss: 0.5073736310005188--->Epoch: 97 | Loss: 0.5073736310005188--->Epoch: 98 | Loss: 0.5073736310005188--->Epoch: 99 | Loss: 0.5073736310005188--->Epoch: 100 | Loss: 0.5073736310005188--->\n",
            "Epoch: 101 | Loss: 0.5073736310005188--->Epoch: 102 | Loss: 0.5073736310005188--->Epoch: 103 | Loss: 0.5073736310005188--->Epoch: 104 | Loss: 0.5073736310005188--->Epoch: 105 | Loss: 0.5073736310005188--->Epoch: 106 | Loss: 0.5073736310005188--->Epoch: 107 | Loss: 0.5073736310005188--->Epoch: 108 | Loss: 0.5073736310005188--->Epoch: 109 | Loss: 0.5073736310005188--->Epoch: 110 | Loss: 0.5073736310005188--->Epoch: 111 | Loss: 0.5073736310005188--->Epoch: 112 | Loss: 0.5073736310005188--->Epoch: 113 | Loss: 0.5073736310005188--->Epoch: 114 | Loss: 0.5073736310005188--->Epoch: 115 | Loss: 0.5073736310005188--->Epoch: 116 | Loss: 0.5073736310005188--->Epoch: 117 | Loss: 0.5073736310005188--->Epoch: 118 | Loss: 0.5073736310005188--->Epoch: 119 | Loss: 0.5073736310005188--->Epoch: 120 | Loss: 0.5073736310005188--->Epoch: 121 | Loss: 0.5073736310005188--->Epoch: 122 | Loss: 0.5073736310005188--->Epoch: 123 | Loss: 0.5073736310005188--->Epoch: 124 | Loss: 0.5073736310005188--->Epoch: 125 | Loss: 0.5073736310005188--->Epoch: 126 | Loss: 0.5073736310005188--->Epoch: 127 | Loss: 0.5073736310005188--->Epoch: 128 | Loss: 0.5073736310005188--->Epoch: 129 | Loss: 0.5073736310005188--->Epoch: 130 | Loss: 0.5073736310005188--->Epoch: 131 | Loss: 0.5073736310005188--->Epoch: 132 | Loss: 0.5073736310005188--->Epoch: 133 | Loss: 0.5073736310005188--->Epoch: 134 | Loss: 0.5073736310005188--->Epoch: 135 | Loss: 0.5073736310005188--->Epoch: 136 | Loss: 0.5073736310005188--->Epoch: 137 | Loss: 0.5073736310005188--->Epoch: 138 | Loss: 0.5073736310005188--->Epoch: 139 | Loss: 0.5073736310005188--->Epoch: 140 | Loss: 0.5073736310005188--->Epoch: 141 | Loss: 0.5073736310005188--->Epoch: 142 | Loss: 0.5073736310005188--->Epoch: 143 | Loss: 0.5073736310005188--->Epoch: 144 | Loss: 0.5073736310005188--->Epoch: 145 | Loss: 0.5073736310005188--->Epoch: 146 | Loss: 0.5073736310005188--->Epoch: 147 | Loss: 0.5073736310005188--->Epoch: 148 | Loss: 0.5073736310005188--->Epoch: 149 | Loss: 0.5073736310005188--->Epoch: 150 | Loss: 0.5073736310005188--->Epoch: 151 | Loss: 0.5073736310005188--->Epoch: 152 | Loss: 0.5073736310005188--->Epoch: 153 | Loss: 0.5073736310005188--->Epoch: 154 | Loss: 0.5073736310005188--->Epoch: 155 | Loss: 0.5073736310005188--->Epoch: 156 | Loss: 0.5073736310005188--->Epoch: 157 | Loss: 0.5073736310005188--->Epoch: 158 | Loss: 0.5073736310005188--->Epoch: 159 | Loss: 0.5073736310005188--->Epoch: 160 | Loss: 0.5073736310005188--->Epoch: 161 | Loss: 0.5073736310005188--->Epoch: 162 | Loss: 0.5073736310005188--->Epoch: 163 | Loss: 0.5073736310005188--->Epoch: 164 | Loss: 0.5073736310005188--->Epoch: 165 | Loss: 0.5073736310005188--->Epoch: 166 | Loss: 0.5073736310005188--->Epoch: 167 | Loss: 0.5073736310005188--->Epoch: 168 | Loss: 0.5073736310005188--->Epoch: 169 | Loss: 0.5073736310005188--->Epoch: 170 | Loss: 0.5073736310005188--->Epoch: 171 | Loss: 0.5073736310005188--->Epoch: 172 | Loss: 0.5073736310005188--->Epoch: 173 | Loss: 0.5073736310005188--->Epoch: 174 | Loss: 0.5073736310005188--->Epoch: 175 | Loss: 0.5073736310005188--->Epoch: 176 | Loss: 0.5073736310005188--->Epoch: 177 | Loss: 0.5073736310005188--->Epoch: 178 | Loss: 0.5073736310005188--->Epoch: 179 | Loss: 0.5073736310005188--->Epoch: 180 | Loss: 0.5073736310005188--->Epoch: 181 | Loss: 0.5073736310005188--->Epoch: 182 | Loss: 0.5073736310005188--->Epoch: 183 | Loss: 0.5073736310005188--->Epoch: 184 | Loss: 0.5073736310005188--->Epoch: 185 | Loss: 0.5073736310005188--->Epoch: 186 | Loss: 0.5073736310005188--->Epoch: 187 | Loss: 0.5073736310005188--->Epoch: 188 | Loss: 0.5073736310005188--->Epoch: 189 | Loss: 0.5073736310005188--->Epoch: 190 | Loss: 0.5073736310005188--->Epoch: 191 | Loss: 0.5073736310005188--->Epoch: 192 | Loss: 0.5073736310005188--->Epoch: 193 | Loss: 0.5073736310005188--->Epoch: 194 | Loss: 0.5073736310005188--->Epoch: 195 | Loss: 0.5073736310005188--->Epoch: 196 | Loss: 0.5073736310005188--->Epoch: 197 | Loss: 0.5073736310005188--->Epoch: 198 | Loss: 0.5073736310005188--->Epoch: 199 | Loss: 0.5073736310005188--->Epoch: 200 | Loss: 0.5073736310005188--->\n",
            "Epoch: 201 | Loss: 0.5073736310005188--->Epoch: 202 | Loss: 0.5073736310005188--->Epoch: 203 | Loss: 0.5073736310005188--->Epoch: 204 | Loss: 0.5073736310005188--->Epoch: 205 | Loss: 0.5073736310005188--->Epoch: 206 | Loss: 0.5073736310005188--->Epoch: 207 | Loss: 0.5073736310005188--->Epoch: 208 | Loss: 0.5073736310005188--->Epoch: 209 | Loss: 0.5073736310005188--->Epoch: 210 | Loss: 0.5073736310005188--->Epoch: 211 | Loss: 0.5073736310005188--->Epoch: 212 | Loss: 0.5073736310005188--->Epoch: 213 | Loss: 0.5073736310005188--->Epoch: 214 | Loss: 0.5073736310005188--->Epoch: 215 | Loss: 0.5073736310005188--->Epoch: 216 | Loss: 0.5073736310005188--->Epoch: 217 | Loss: 0.5073736310005188--->Epoch: 218 | Loss: 0.5073736310005188--->Epoch: 219 | Loss: 0.5073736310005188--->Epoch: 220 | Loss: 0.5073736310005188--->Epoch: 221 | Loss: 0.5073736310005188--->Epoch: 222 | Loss: 0.5073736310005188--->Epoch: 223 | Loss: 0.5073736310005188--->Epoch: 224 | Loss: 0.5073736310005188--->Epoch: 225 | Loss: 0.5073736310005188--->Epoch: 226 | Loss: 0.5073736310005188--->Epoch: 227 | Loss: 0.5073736310005188--->Epoch: 228 | Loss: 0.5073736310005188--->Epoch: 229 | Loss: 0.5073736310005188--->Epoch: 230 | Loss: 0.5073736310005188--->Epoch: 231 | Loss: 0.5073736310005188--->Epoch: 232 | Loss: 0.5073736310005188--->Epoch: 233 | Loss: 0.5073736310005188--->Epoch: 234 | Loss: 0.5073736310005188--->Epoch: 235 | Loss: 0.5073736310005188--->Epoch: 236 | Loss: 0.5073736310005188--->Epoch: 237 | Loss: 0.5073736310005188--->Epoch: 238 | Loss: 0.5073736310005188--->Epoch: 239 | Loss: 0.5073736310005188--->Epoch: 240 | Loss: 0.5073736310005188--->Epoch: 241 | Loss: 0.5073736310005188--->Epoch: 242 | Loss: 0.5073736310005188--->Epoch: 243 | Loss: 0.5073736310005188--->Epoch: 244 | Loss: 0.5073736310005188--->Epoch: 245 | Loss: 0.5073736310005188--->Epoch: 246 | Loss: 0.5073736310005188--->Epoch: 247 | Loss: 0.5073736310005188--->Epoch: 248 | Loss: 0.5073736310005188--->Epoch: 249 | Loss: 0.5073736310005188--->Epoch: 250 | Loss: 0.5073736310005188--->Epoch: 251 | Loss: 0.5073736310005188--->Epoch: 252 | Loss: 0.5073736310005188--->Epoch: 253 | Loss: 0.5073736310005188--->Epoch: 254 | Loss: 0.5073736310005188--->Epoch: 255 | Loss: 0.5073736310005188--->Epoch: 256 | Loss: 0.5073736310005188--->Epoch: 257 | Loss: 0.5073736310005188--->Epoch: 258 | Loss: 0.5073736310005188--->Epoch: 259 | Loss: 0.5073736310005188--->Epoch: 260 | Loss: 0.5073736310005188--->Epoch: 261 | Loss: 0.5073736310005188--->Epoch: 262 | Loss: 0.5073736310005188--->Epoch: 263 | Loss: 0.5073736310005188--->Epoch: 264 | Loss: 0.5073736310005188--->Epoch: 265 | Loss: 0.5073736310005188--->Epoch: 266 | Loss: 0.5073736310005188--->Epoch: 267 | Loss: 0.5073736310005188--->Epoch: 268 | Loss: 0.5073736310005188--->Epoch: 269 | Loss: 0.5073736310005188--->Epoch: 270 | Loss: 0.5073736310005188--->Epoch: 271 | Loss: 0.5073736310005188--->Epoch: 272 | Loss: 0.5073736310005188--->Epoch: 273 | Loss: 0.5073736310005188--->Epoch: 274 | Loss: 0.5073736310005188--->Epoch: 275 | Loss: 0.5073736310005188--->Epoch: 276 | Loss: 0.5073736310005188--->Epoch: 277 | Loss: 0.5073736310005188--->Epoch: 278 | Loss: 0.5073736310005188--->Epoch: 279 | Loss: 0.5073736310005188--->Epoch: 280 | Loss: 0.5073736310005188--->Epoch: 281 | Loss: 0.5073736310005188--->Epoch: 282 | Loss: 0.5073736310005188--->Epoch: 283 | Loss: 0.5073736310005188--->Epoch: 284 | Loss: 0.5073736310005188--->Epoch: 285 | Loss: 0.5073736310005188--->Epoch: 286 | Loss: 0.5073736310005188--->Epoch: 287 | Loss: 0.5073736310005188--->Epoch: 288 | Loss: 0.5073736310005188--->Epoch: 289 | Loss: 0.5073736310005188--->Epoch: 290 | Loss: 0.5073736310005188--->Epoch: 291 | Loss: 0.5073736310005188--->Epoch: 292 | Loss: 0.5073736310005188--->Epoch: 293 | Loss: 0.5073736310005188--->Epoch: 294 | Loss: 0.5073736310005188--->Epoch: 295 | Loss: 0.5073736310005188--->Epoch: 296 | Loss: 0.5073736310005188--->Epoch: 297 | Loss: 0.5073736310005188--->Epoch: 298 | Loss: 0.5073736310005188--->Epoch: 299 | Loss: 0.5073736310005188--->Epoch: 300 | Loss: 0.5073736310005188--->\n",
            "Epoch: 301 | Loss: 0.5073736310005188--->Epoch: 302 | Loss: 0.5073736310005188--->Epoch: 303 | Loss: 0.5073736310005188--->Epoch: 304 | Loss: 0.5073736310005188--->Epoch: 305 | Loss: 0.5073736310005188--->Epoch: 306 | Loss: 0.5073736310005188--->Epoch: 307 | Loss: 0.5073736310005188--->Epoch: 308 | Loss: 0.5073736310005188--->Epoch: 309 | Loss: 0.5073736310005188--->Epoch: 310 | Loss: 0.5073736310005188--->Epoch: 311 | Loss: 0.5073736310005188--->Epoch: 312 | Loss: 0.5073736310005188--->Epoch: 313 | Loss: 0.5073736310005188--->Epoch: 314 | Loss: 0.5073736310005188--->Epoch: 315 | Loss: 0.5073736310005188--->Epoch: 316 | Loss: 0.5073736310005188--->Epoch: 317 | Loss: 0.5073736310005188--->Epoch: 318 | Loss: 0.5073736310005188--->Epoch: 319 | Loss: 0.5073736310005188--->Epoch: 320 | Loss: 0.5073736310005188--->Epoch: 321 | Loss: 0.5073736310005188--->Epoch: 322 | Loss: 0.5073736310005188--->Epoch: 323 | Loss: 0.5073736310005188--->Epoch: 324 | Loss: 0.5073736310005188--->Epoch: 325 | Loss: 0.5073736310005188--->Epoch: 326 | Loss: 0.5073736310005188--->Epoch: 327 | Loss: 0.5073736310005188--->Epoch: 328 | Loss: 0.5073736310005188--->Epoch: 329 | Loss: 0.5073736310005188--->Epoch: 330 | Loss: 0.5073736310005188--->Epoch: 331 | Loss: 0.5073736310005188--->Epoch: 332 | Loss: 0.5073736310005188--->Epoch: 333 | Loss: 0.5073736310005188--->Epoch: 334 | Loss: 0.5073736310005188--->Epoch: 335 | Loss: 0.5073736310005188--->Epoch: 336 | Loss: 0.5073736310005188--->Epoch: 337 | Loss: 0.5073736310005188--->Epoch: 338 | Loss: 0.5073736310005188--->Epoch: 339 | Loss: 0.5073736310005188--->Epoch: 340 | Loss: 0.5073736310005188--->Epoch: 341 | Loss: 0.5073736310005188--->Epoch: 342 | Loss: 0.5073736310005188--->Epoch: 343 | Loss: 0.5073736310005188--->Epoch: 344 | Loss: 0.5073736310005188--->Epoch: 345 | Loss: 0.5073736310005188--->Epoch: 346 | Loss: 0.5073736310005188--->Epoch: 347 | Loss: 0.5073736310005188--->Epoch: 348 | Loss: 0.5073736310005188--->Epoch: 349 | Loss: 0.5073736310005188--->Epoch: 350 | Loss: 0.5073736310005188--->Epoch: 351 | Loss: 0.5073736310005188--->Epoch: 352 | Loss: 0.5073736310005188--->Epoch: 353 | Loss: 0.5073736310005188--->Epoch: 354 | Loss: 0.5073736310005188--->Epoch: 355 | Loss: 0.5073736310005188--->Epoch: 356 | Loss: 0.5073736310005188--->Epoch: 357 | Loss: 0.5073736310005188--->Epoch: 358 | Loss: 0.5073736310005188--->Epoch: 359 | Loss: 0.5073736310005188--->Epoch: 360 | Loss: 0.5073736310005188--->Epoch: 361 | Loss: 0.5073736310005188--->Epoch: 362 | Loss: 0.5073736310005188--->Epoch: 363 | Loss: 0.5073736310005188--->Epoch: 364 | Loss: 0.5073736310005188--->Epoch: 365 | Loss: 0.5073736310005188--->Epoch: 366 | Loss: 0.5073736310005188--->Epoch: 367 | Loss: 0.5073736310005188--->Epoch: 368 | Loss: 0.5073736310005188--->Epoch: 369 | Loss: 0.5073736310005188--->Epoch: 370 | Loss: 0.5073736310005188--->Epoch: 371 | Loss: 0.5073736310005188--->Epoch: 372 | Loss: 0.5073736310005188--->Epoch: 373 | Loss: 0.5073736310005188--->Epoch: 374 | Loss: 0.5073736310005188--->Epoch: 375 | Loss: 0.5073736310005188--->Epoch: 376 | Loss: 0.5073736310005188--->Epoch: 377 | Loss: 0.5073736310005188--->Epoch: 378 | Loss: 0.5073736310005188--->Epoch: 379 | Loss: 0.5073736310005188--->Epoch: 380 | Loss: 0.5073736310005188--->Epoch: 381 | Loss: 0.5073736310005188--->Epoch: 382 | Loss: 0.5073736310005188--->Epoch: 383 | Loss: 0.5073736310005188--->Epoch: 384 | Loss: 0.5073736310005188--->Epoch: 385 | Loss: 0.5073736310005188--->Epoch: 386 | Loss: 0.5073736310005188--->Epoch: 387 | Loss: 0.5073736310005188--->Epoch: 388 | Loss: 0.5073736310005188--->Epoch: 389 | Loss: 0.5073736310005188--->Epoch: 390 | Loss: 0.5073736310005188--->Epoch: 391 | Loss: 0.5073736310005188--->Epoch: 392 | Loss: 0.5073736310005188--->Epoch: 393 | Loss: 0.5073736310005188--->Epoch: 394 | Loss: 0.5073736310005188--->Epoch: 395 | Loss: 0.5073736310005188--->Epoch: 396 | Loss: 0.5073736310005188--->Epoch: 397 | Loss: 0.5073736310005188--->Epoch: 398 | Loss: 0.5073736310005188--->Epoch: 399 | Loss: 0.5073736310005188--->Epoch: 400 | Loss: 0.5073736310005188--->\n",
            "Epoch: 401 | Loss: 0.5073736310005188--->Epoch: 402 | Loss: 0.5073736310005188--->Epoch: 403 | Loss: 0.5073736310005188--->Epoch: 404 | Loss: 0.5073736310005188--->Epoch: 405 | Loss: 0.5073736310005188--->Epoch: 406 | Loss: 0.5073736310005188--->Epoch: 407 | Loss: 0.5073736310005188--->Epoch: 408 | Loss: 0.5073736310005188--->Epoch: 409 | Loss: 0.5073736310005188--->Epoch: 410 | Loss: 0.5073736310005188--->Epoch: 411 | Loss: 0.5073736310005188--->Epoch: 412 | Loss: 0.5073736310005188--->Epoch: 413 | Loss: 0.5073736310005188--->Epoch: 414 | Loss: 0.5073736310005188--->Epoch: 415 | Loss: 0.5073736310005188--->Epoch: 416 | Loss: 0.5073736310005188--->Epoch: 417 | Loss: 0.5073736310005188--->Epoch: 418 | Loss: 0.5073736310005188--->Epoch: 419 | Loss: 0.5073736310005188--->Epoch: 420 | Loss: 0.5073736310005188--->Epoch: 421 | Loss: 0.5073736310005188--->Epoch: 422 | Loss: 0.5073736310005188--->Epoch: 423 | Loss: 0.5073736310005188--->Epoch: 424 | Loss: 0.5073736310005188--->Epoch: 425 | Loss: 0.5073736310005188--->Epoch: 426 | Loss: 0.5073736310005188--->Epoch: 427 | Loss: 0.5073736310005188--->Epoch: 428 | Loss: 0.5073736310005188--->Epoch: 429 | Loss: 0.5073736310005188--->Epoch: 430 | Loss: 0.5073736310005188--->Epoch: 431 | Loss: 0.5073736310005188--->Epoch: 432 | Loss: 0.5073736310005188--->Epoch: 433 | Loss: 0.5073736310005188--->Epoch: 434 | Loss: 0.5073736310005188--->Epoch: 435 | Loss: 0.5073736310005188--->Epoch: 436 | Loss: 0.5073736310005188--->Epoch: 437 | Loss: 0.5073736310005188--->Epoch: 438 | Loss: 0.5073736310005188--->Epoch: 439 | Loss: 0.5073736310005188--->Epoch: 440 | Loss: 0.5073736310005188--->Epoch: 441 | Loss: 0.5073736310005188--->Epoch: 442 | Loss: 0.5073736310005188--->Epoch: 443 | Loss: 0.5073736310005188--->Epoch: 444 | Loss: 0.5073736310005188--->Epoch: 445 | Loss: 0.5073736310005188--->Epoch: 446 | Loss: 0.5073736310005188--->Epoch: 447 | Loss: 0.5073736310005188--->Epoch: 448 | Loss: 0.5073736310005188--->Epoch: 449 | Loss: 0.5073736310005188--->Epoch: 450 | Loss: 0.5073736310005188--->Epoch: 451 | Loss: 0.5073736310005188--->Epoch: 452 | Loss: 0.5073736310005188--->Epoch: 453 | Loss: 0.5073736310005188--->Epoch: 454 | Loss: 0.5073736310005188--->Epoch: 455 | Loss: 0.5073736310005188--->Epoch: 456 | Loss: 0.5073736310005188--->Epoch: 457 | Loss: 0.5073736310005188--->Epoch: 458 | Loss: 0.5073736310005188--->Epoch: 459 | Loss: 0.5073736310005188--->Epoch: 460 | Loss: 0.5073736310005188--->Epoch: 461 | Loss: 0.5073736310005188--->Epoch: 462 | Loss: 0.5073736310005188--->Epoch: 463 | Loss: 0.5073736310005188--->Epoch: 464 | Loss: 0.5073736310005188--->Epoch: 465 | Loss: 0.5073736310005188--->Epoch: 466 | Loss: 0.5073736310005188--->Epoch: 467 | Loss: 0.5073736310005188--->Epoch: 468 | Loss: 0.5073736310005188--->Epoch: 469 | Loss: 0.5073736310005188--->Epoch: 470 | Loss: 0.5073736310005188--->Epoch: 471 | Loss: 0.5073736310005188--->Epoch: 472 | Loss: 0.5073736310005188--->Epoch: 473 | Loss: 0.5073736310005188--->Epoch: 474 | Loss: 0.5073736310005188--->Epoch: 475 | Loss: 0.5073736310005188--->Epoch: 476 | Loss: 0.5073736310005188--->Epoch: 477 | Loss: 0.5073736310005188--->Epoch: 478 | Loss: 0.5073736310005188--->Epoch: 479 | Loss: 0.5073736310005188--->Epoch: 480 | Loss: 0.5073736310005188--->Epoch: 481 | Loss: 0.5073736310005188--->Epoch: 482 | Loss: 0.5073736310005188--->Epoch: 483 | Loss: 0.5073736310005188--->Epoch: 484 | Loss: 0.5073736310005188--->Epoch: 485 | Loss: 0.5073736310005188--->Epoch: 486 | Loss: 0.5073736310005188--->Epoch: 487 | Loss: 0.5073736310005188--->Epoch: 488 | Loss: 0.5073736310005188--->Epoch: 489 | Loss: 0.5073736310005188--->Epoch: 490 | Loss: 0.5073736310005188--->Epoch: 491 | Loss: 0.5073736310005188--->Epoch: 492 | Loss: 0.5073736310005188--->Epoch: 493 | Loss: 0.5073736310005188--->Epoch: 494 | Loss: 0.5073736310005188--->Epoch: 495 | Loss: 0.5073736310005188--->Epoch: 496 | Loss: 0.5073736310005188--->Epoch: 497 | Loss: 0.5073736310005188--->Epoch: 498 | Loss: 0.5073736310005188--->Epoch: 499 | Loss: 0.5073736310005188--->Epoch: 500 | Loss: 0.5073736310005188--->\n",
            "Epoch: 501 | Loss: 0.5073736310005188--->Epoch: 502 | Loss: 0.5073736310005188--->Epoch: 503 | Loss: 0.5073736310005188--->Epoch: 504 | Loss: 0.5073736310005188--->Epoch: 505 | Loss: 0.5073736310005188--->Epoch: 506 | Loss: 0.5073736310005188--->Epoch: 507 | Loss: 0.5073736310005188--->Epoch: 508 | Loss: 0.5073736310005188--->Epoch: 509 | Loss: 0.5073736310005188--->Epoch: 510 | Loss: 0.5073736310005188--->Epoch: 511 | Loss: 0.5073736310005188--->Epoch: 512 | Loss: 0.5073736310005188--->Epoch: 513 | Loss: 0.5073736310005188--->Epoch: 514 | Loss: 0.5073736310005188--->Epoch: 515 | Loss: 0.5073736310005188--->Epoch: 516 | Loss: 0.5073736310005188--->Epoch: 517 | Loss: 0.5073736310005188--->Epoch: 518 | Loss: 0.5073736310005188--->Epoch: 519 | Loss: 0.5073736310005188--->Epoch: 520 | Loss: 0.5073736310005188--->Epoch: 521 | Loss: 0.5073736310005188--->Epoch: 522 | Loss: 0.5073736310005188--->Epoch: 523 | Loss: 0.5073736310005188--->Epoch: 524 | Loss: 0.5073736310005188--->Epoch: 525 | Loss: 0.5073736310005188--->Epoch: 526 | Loss: 0.5073736310005188--->Epoch: 527 | Loss: 0.5073736310005188--->Epoch: 528 | Loss: 0.5073736310005188--->Epoch: 529 | Loss: 0.5073736310005188--->Epoch: 530 | Loss: 0.5073736310005188--->Epoch: 531 | Loss: 0.5073736310005188--->Epoch: 532 | Loss: 0.5073736310005188--->Epoch: 533 | Loss: 0.5073736310005188--->Epoch: 534 | Loss: 0.5073736310005188--->Epoch: 535 | Loss: 0.5073736310005188--->Epoch: 536 | Loss: 0.5073736310005188--->Epoch: 537 | Loss: 0.5073736310005188--->Epoch: 538 | Loss: 0.5073736310005188--->Epoch: 539 | Loss: 0.5073736310005188--->Epoch: 540 | Loss: 0.5073736310005188--->Epoch: 541 | Loss: 0.5073736310005188--->Epoch: 542 | Loss: 0.5073736310005188--->Epoch: 543 | Loss: 0.5073736310005188--->Epoch: 544 | Loss: 0.5073736310005188--->Epoch: 545 | Loss: 0.5073736310005188--->Epoch: 546 | Loss: 0.5073736310005188--->Epoch: 547 | Loss: 0.5073736310005188--->Epoch: 548 | Loss: 0.5073736310005188--->Epoch: 549 | Loss: 0.5073736310005188--->Epoch: 550 | Loss: 0.5073736310005188--->Epoch: 551 | Loss: 0.5073736310005188--->Epoch: 552 | Loss: 0.5073736310005188--->Epoch: 553 | Loss: 0.5073736310005188--->Epoch: 554 | Loss: 0.5073736310005188--->Epoch: 555 | Loss: 0.5073736310005188--->Epoch: 556 | Loss: 0.5073736310005188--->Epoch: 557 | Loss: 0.5073736310005188--->Epoch: 558 | Loss: 0.5073736310005188--->Epoch: 559 | Loss: 0.5073736310005188--->Epoch: 560 | Loss: 0.5073736310005188--->Epoch: 561 | Loss: 0.5073736310005188--->Epoch: 562 | Loss: 0.5073736310005188--->Epoch: 563 | Loss: 0.5073736310005188--->Epoch: 564 | Loss: 0.5073736310005188--->Epoch: 565 | Loss: 0.5073736310005188--->Epoch: 566 | Loss: 0.5073736310005188--->Epoch: 567 | Loss: 0.5073736310005188--->Epoch: 568 | Loss: 0.5073736310005188--->Epoch: 569 | Loss: 0.5073736310005188--->Epoch: 570 | Loss: 0.5073736310005188--->Epoch: 571 | Loss: 0.5073736310005188--->Epoch: 572 | Loss: 0.5073736310005188--->Epoch: 573 | Loss: 0.5073736310005188--->Epoch: 574 | Loss: 0.5073736310005188--->Epoch: 575 | Loss: 0.5073736310005188--->Epoch: 576 | Loss: 0.5073736310005188--->Epoch: 577 | Loss: 0.5073736310005188--->Epoch: 578 | Loss: 0.5073736310005188--->Epoch: 579 | Loss: 0.5073736310005188--->Epoch: 580 | Loss: 0.5073736310005188--->Epoch: 581 | Loss: 0.5073736310005188--->Epoch: 582 | Loss: 0.5073736310005188--->Epoch: 583 | Loss: 0.5073736310005188--->Epoch: 584 | Loss: 0.5073736310005188--->Epoch: 585 | Loss: 0.5073736310005188--->Epoch: 586 | Loss: 0.5073736310005188--->Epoch: 587 | Loss: 0.5073736310005188--->Epoch: 588 | Loss: 0.5073736310005188--->Epoch: 589 | Loss: 0.5073736310005188--->Epoch: 590 | Loss: 0.5073736310005188--->Epoch: 591 | Loss: 0.5073736310005188--->Epoch: 592 | Loss: 0.5073736310005188--->Epoch: 593 | Loss: 0.5073736310005188--->Epoch: 594 | Loss: 0.5073736310005188--->Epoch: 595 | Loss: 0.5073736310005188--->Epoch: 596 | Loss: 0.5073736310005188--->Epoch: 597 | Loss: 0.5073736310005188--->Epoch: 598 | Loss: 0.5073736310005188--->Epoch: 599 | Loss: 0.5073736310005188--->Epoch: 600 | Loss: 0.5073736310005188--->\n",
            "Epoch: 601 | Loss: 0.5073736310005188--->Epoch: 602 | Loss: 0.5073736310005188--->Epoch: 603 | Loss: 0.5073736310005188--->Epoch: 604 | Loss: 0.5073736310005188--->Epoch: 605 | Loss: 0.5073736310005188--->Epoch: 606 | Loss: 0.5073736310005188--->Epoch: 607 | Loss: 0.5073736310005188--->Epoch: 608 | Loss: 0.5073736310005188--->Epoch: 609 | Loss: 0.5073736310005188--->Epoch: 610 | Loss: 0.5073736310005188--->Epoch: 611 | Loss: 0.5073736310005188--->Epoch: 612 | Loss: 0.5073736310005188--->Epoch: 613 | Loss: 0.5073736310005188--->Epoch: 614 | Loss: 0.5073736310005188--->Epoch: 615 | Loss: 0.5073736310005188--->Epoch: 616 | Loss: 0.5073736310005188--->Epoch: 617 | Loss: 0.5073736310005188--->Epoch: 618 | Loss: 0.5073736310005188--->Epoch: 619 | Loss: 0.5073736310005188--->Epoch: 620 | Loss: 0.5073736310005188--->Epoch: 621 | Loss: 0.5073736310005188--->Epoch: 622 | Loss: 0.5073736310005188--->Epoch: 623 | Loss: 0.5073736310005188--->Epoch: 624 | Loss: 0.5073736310005188--->Epoch: 625 | Loss: 0.5073736310005188--->Epoch: 626 | Loss: 0.5073736310005188--->Epoch: 627 | Loss: 0.5073736310005188--->Epoch: 628 | Loss: 0.5073736310005188--->Epoch: 629 | Loss: 0.5073736310005188--->Epoch: 630 | Loss: 0.5073736310005188--->Epoch: 631 | Loss: 0.5073736310005188--->Epoch: 632 | Loss: 0.5073736310005188--->Epoch: 633 | Loss: 0.5073736310005188--->Epoch: 634 | Loss: 0.5073736310005188--->Epoch: 635 | Loss: 0.5073736310005188--->Epoch: 636 | Loss: 0.5073736310005188--->Epoch: 637 | Loss: 0.5073736310005188--->Epoch: 638 | Loss: 0.5073736310005188--->Epoch: 639 | Loss: 0.5073736310005188--->Epoch: 640 | Loss: 0.5073736310005188--->Epoch: 641 | Loss: 0.5073736310005188--->Epoch: 642 | Loss: 0.5073736310005188--->Epoch: 643 | Loss: 0.5073736310005188--->Epoch: 644 | Loss: 0.5073736310005188--->Epoch: 645 | Loss: 0.5073736310005188--->Epoch: 646 | Loss: 0.5073736310005188--->Epoch: 647 | Loss: 0.5073736310005188--->Epoch: 648 | Loss: 0.5073736310005188--->Epoch: 649 | Loss: 0.5073736310005188--->Epoch: 650 | Loss: 0.5073736310005188--->Epoch: 651 | Loss: 0.5073736310005188--->Epoch: 652 | Loss: 0.5073736310005188--->Epoch: 653 | Loss: 0.5073736310005188--->Epoch: 654 | Loss: 0.5073736310005188--->Epoch: 655 | Loss: 0.5073736310005188--->Epoch: 656 | Loss: 0.5073736310005188--->Epoch: 657 | Loss: 0.5073736310005188--->Epoch: 658 | Loss: 0.5073736310005188--->Epoch: 659 | Loss: 0.5073736310005188--->Epoch: 660 | Loss: 0.5073736310005188--->Epoch: 661 | Loss: 0.5073736310005188--->Epoch: 662 | Loss: 0.5073736310005188--->Epoch: 663 | Loss: 0.5073736310005188--->Epoch: 664 | Loss: 0.5073736310005188--->Epoch: 665 | Loss: 0.5073736310005188--->Epoch: 666 | Loss: 0.5073736310005188--->Epoch: 667 | Loss: 0.5073736310005188--->Epoch: 668 | Loss: 0.5073736310005188--->Epoch: 669 | Loss: 0.5073736310005188--->Epoch: 670 | Loss: 0.5073736310005188--->Epoch: 671 | Loss: 0.5073736310005188--->Epoch: 672 | Loss: 0.5073736310005188--->Epoch: 673 | Loss: 0.5073736310005188--->Epoch: 674 | Loss: 0.5073736310005188--->Epoch: 675 | Loss: 0.5073736310005188--->Epoch: 676 | Loss: 0.5073736310005188--->Epoch: 677 | Loss: 0.5073736310005188--->Epoch: 678 | Loss: 0.5073736310005188--->Epoch: 679 | Loss: 0.5073736310005188--->Epoch: 680 | Loss: 0.5073736310005188--->Epoch: 681 | Loss: 0.5073736310005188--->Epoch: 682 | Loss: 0.5073736310005188--->Epoch: 683 | Loss: 0.5073736310005188--->Epoch: 684 | Loss: 0.5073736310005188--->Epoch: 685 | Loss: 0.5073736310005188--->Epoch: 686 | Loss: 0.5073736310005188--->Epoch: 687 | Loss: 0.5073736310005188--->Epoch: 688 | Loss: 0.5073736310005188--->Epoch: 689 | Loss: 0.5073736310005188--->Epoch: 690 | Loss: 0.5073736310005188--->Epoch: 691 | Loss: 0.5073736310005188--->Epoch: 692 | Loss: 0.5073736310005188--->Epoch: 693 | Loss: 0.5073736310005188--->Epoch: 694 | Loss: 0.5073736310005188--->Epoch: 695 | Loss: 0.5073736310005188--->Epoch: 696 | Loss: 0.5073736310005188--->Epoch: 697 | Loss: 0.5073736310005188--->Epoch: 698 | Loss: 0.5073736310005188--->Epoch: 699 | Loss: 0.5073736310005188--->Epoch: 700 | Loss: 0.5073736310005188--->\n",
            "Epoch: 701 | Loss: 0.5073736310005188--->Epoch: 702 | Loss: 0.5073736310005188--->Epoch: 703 | Loss: 0.5073736310005188--->Epoch: 704 | Loss: 0.5073736310005188--->Epoch: 705 | Loss: 0.5073736310005188--->Epoch: 706 | Loss: 0.5073736310005188--->Epoch: 707 | Loss: 0.5073736310005188--->Epoch: 708 | Loss: 0.5073736310005188--->Epoch: 709 | Loss: 0.5073736310005188--->Epoch: 710 | Loss: 0.5073736310005188--->Epoch: 711 | Loss: 0.5073736310005188--->Epoch: 712 | Loss: 0.5073736310005188--->Epoch: 713 | Loss: 0.5073736310005188--->Epoch: 714 | Loss: 0.5073736310005188--->Epoch: 715 | Loss: 0.5073736310005188--->Epoch: 716 | Loss: 0.5073736310005188--->Epoch: 717 | Loss: 0.5073736310005188--->Epoch: 718 | Loss: 0.5073736310005188--->Epoch: 719 | Loss: 0.5073736310005188--->Epoch: 720 | Loss: 0.5073736310005188--->Epoch: 721 | Loss: 0.5073736310005188--->Epoch: 722 | Loss: 0.5073736310005188--->Epoch: 723 | Loss: 0.5073736310005188--->Epoch: 724 | Loss: 0.5073736310005188--->Epoch: 725 | Loss: 0.5073736310005188--->Epoch: 726 | Loss: 0.5073736310005188--->Epoch: 727 | Loss: 0.5073736310005188--->Epoch: 728 | Loss: 0.5073736310005188--->Epoch: 729 | Loss: 0.5073736310005188--->Epoch: 730 | Loss: 0.5073736310005188--->Epoch: 731 | Loss: 0.5073736310005188--->Epoch: 732 | Loss: 0.5073736310005188--->Epoch: 733 | Loss: 0.5073736310005188--->Epoch: 734 | Loss: 0.5073736310005188--->Epoch: 735 | Loss: 0.5073736310005188--->Epoch: 736 | Loss: 0.5073736310005188--->Epoch: 737 | Loss: 0.5073736310005188--->Epoch: 738 | Loss: 0.5073736310005188--->Epoch: 739 | Loss: 0.5073736310005188--->Epoch: 740 | Loss: 0.5073736310005188--->Epoch: 741 | Loss: 0.5073736310005188--->Epoch: 742 | Loss: 0.5073736310005188--->Epoch: 743 | Loss: 0.5073736310005188--->Epoch: 744 | Loss: 0.5073736310005188--->Epoch: 745 | Loss: 0.5073736310005188--->Epoch: 746 | Loss: 0.5073736310005188--->Epoch: 747 | Loss: 0.5073736310005188--->Epoch: 748 | Loss: 0.5073736310005188--->Epoch: 749 | Loss: 0.5073736310005188--->Epoch: 750 | Loss: 0.5073736310005188--->Epoch: 751 | Loss: 0.5073736310005188--->Epoch: 752 | Loss: 0.5073736310005188--->Epoch: 753 | Loss: 0.5073736310005188--->Epoch: 754 | Loss: 0.5073736310005188--->Epoch: 755 | Loss: 0.5073736310005188--->Epoch: 756 | Loss: 0.5073736310005188--->Epoch: 757 | Loss: 0.5073736310005188--->Epoch: 758 | Loss: 0.5073736310005188--->Epoch: 759 | Loss: 0.5073736310005188--->Epoch: 760 | Loss: 0.5073736310005188--->Epoch: 761 | Loss: 0.5073736310005188--->Epoch: 762 | Loss: 0.5073736310005188--->Epoch: 763 | Loss: 0.5073736310005188--->Epoch: 764 | Loss: 0.5073736310005188--->Epoch: 765 | Loss: 0.5073736310005188--->Epoch: 766 | Loss: 0.5073736310005188--->Epoch: 767 | Loss: 0.5073736310005188--->Epoch: 768 | Loss: 0.5073736310005188--->Epoch: 769 | Loss: 0.5073736310005188--->Epoch: 770 | Loss: 0.5073736310005188--->Epoch: 771 | Loss: 0.5073736310005188--->Epoch: 772 | Loss: 0.5073736310005188--->Epoch: 773 | Loss: 0.5073736310005188--->Epoch: 774 | Loss: 0.5073736310005188--->Epoch: 775 | Loss: 0.5073736310005188--->Epoch: 776 | Loss: 0.5073736310005188--->Epoch: 777 | Loss: 0.5073736310005188--->Epoch: 778 | Loss: 0.5073736310005188--->Epoch: 779 | Loss: 0.5073736310005188--->Epoch: 780 | Loss: 0.5073736310005188--->Epoch: 781 | Loss: 0.5073736310005188--->Epoch: 782 | Loss: 0.5073736310005188--->Epoch: 783 | Loss: 0.5073736310005188--->Epoch: 784 | Loss: 0.5073736310005188--->Epoch: 785 | Loss: 0.5073736310005188--->Epoch: 786 | Loss: 0.5073736310005188--->Epoch: 787 | Loss: 0.5073736310005188--->Epoch: 788 | Loss: 0.5073736310005188--->Epoch: 789 | Loss: 0.5073736310005188--->Epoch: 790 | Loss: 0.5073736310005188--->Epoch: 791 | Loss: 0.5073736310005188--->Epoch: 792 | Loss: 0.5073736310005188--->Epoch: 793 | Loss: 0.5073736310005188--->Epoch: 794 | Loss: 0.5073736310005188--->Epoch: 795 | Loss: 0.5073736310005188--->Epoch: 796 | Loss: 0.5073736310005188--->Epoch: 797 | Loss: 0.5073736310005188--->Epoch: 798 | Loss: 0.5073736310005188--->Epoch: 799 | Loss: 0.5073736310005188--->Epoch: 800 | Loss: 0.5073736310005188--->\n",
            "Epoch: 801 | Loss: 0.5073736310005188--->Epoch: 802 | Loss: 0.5073736310005188--->Epoch: 803 | Loss: 0.5073736310005188--->Epoch: 804 | Loss: 0.5073736310005188--->Epoch: 805 | Loss: 0.5073736310005188--->Epoch: 806 | Loss: 0.5073736310005188--->Epoch: 807 | Loss: 0.5073736310005188--->Epoch: 808 | Loss: 0.5073736310005188--->Epoch: 809 | Loss: 0.5073736310005188--->Epoch: 810 | Loss: 0.5073736310005188--->Epoch: 811 | Loss: 0.5073736310005188--->Epoch: 812 | Loss: 0.5073736310005188--->Epoch: 813 | Loss: 0.5073736310005188--->Epoch: 814 | Loss: 0.5073736310005188--->Epoch: 815 | Loss: 0.5073736310005188--->Epoch: 816 | Loss: 0.5073736310005188--->Epoch: 817 | Loss: 0.5073736310005188--->Epoch: 818 | Loss: 0.5073736310005188--->Epoch: 819 | Loss: 0.5073736310005188--->Epoch: 820 | Loss: 0.5073736310005188--->Epoch: 821 | Loss: 0.5073736310005188--->Epoch: 822 | Loss: 0.5073736310005188--->Epoch: 823 | Loss: 0.5073736310005188--->Epoch: 824 | Loss: 0.5073736310005188--->Epoch: 825 | Loss: 0.5073736310005188--->Epoch: 826 | Loss: 0.5073736310005188--->Epoch: 827 | Loss: 0.5073736310005188--->Epoch: 828 | Loss: 0.5073736310005188--->Epoch: 829 | Loss: 0.5073736310005188--->Epoch: 830 | Loss: 0.5073736310005188--->Epoch: 831 | Loss: 0.5073736310005188--->Epoch: 832 | Loss: 0.5073736310005188--->Epoch: 833 | Loss: 0.5073736310005188--->Epoch: 834 | Loss: 0.5073736310005188--->Epoch: 835 | Loss: 0.5073736310005188--->Epoch: 836 | Loss: 0.5073736310005188--->Epoch: 837 | Loss: 0.5073736310005188--->Epoch: 838 | Loss: 0.5073736310005188--->Epoch: 839 | Loss: 0.5073736310005188--->Epoch: 840 | Loss: 0.5073736310005188--->Epoch: 841 | Loss: 0.5073736310005188--->Epoch: 842 | Loss: 0.5073736310005188--->Epoch: 843 | Loss: 0.5073736310005188--->Epoch: 844 | Loss: 0.5073736310005188--->Epoch: 845 | Loss: 0.5073736310005188--->Epoch: 846 | Loss: 0.5073736310005188--->Epoch: 847 | Loss: 0.5073736310005188--->Epoch: 848 | Loss: 0.5073736310005188--->Epoch: 849 | Loss: 0.5073736310005188--->Epoch: 850 | Loss: 0.5073736310005188--->Epoch: 851 | Loss: 0.5073736310005188--->Epoch: 852 | Loss: 0.5073736310005188--->Epoch: 853 | Loss: 0.5073736310005188--->Epoch: 854 | Loss: 0.5073736310005188--->Epoch: 855 | Loss: 0.5073736310005188--->Epoch: 856 | Loss: 0.5073736310005188--->Epoch: 857 | Loss: 0.5073736310005188--->Epoch: 858 | Loss: 0.5073736310005188--->Epoch: 859 | Loss: 0.5073736310005188--->Epoch: 860 | Loss: 0.5073736310005188--->Epoch: 861 | Loss: 0.5073736310005188--->Epoch: 862 | Loss: 0.5073736310005188--->Epoch: 863 | Loss: 0.5073736310005188--->Epoch: 864 | Loss: 0.5073736310005188--->Epoch: 865 | Loss: 0.5073736310005188--->Epoch: 866 | Loss: 0.5073736310005188--->Epoch: 867 | Loss: 0.5073736310005188--->Epoch: 868 | Loss: 0.5073736310005188--->Epoch: 869 | Loss: 0.5073736310005188--->Epoch: 870 | Loss: 0.5073736310005188--->Epoch: 871 | Loss: 0.5073736310005188--->Epoch: 872 | Loss: 0.5073736310005188--->Epoch: 873 | Loss: 0.5073736310005188--->Epoch: 874 | Loss: 0.5073736310005188--->Epoch: 875 | Loss: 0.5073736310005188--->Epoch: 876 | Loss: 0.5073736310005188--->Epoch: 877 | Loss: 0.5073736310005188--->Epoch: 878 | Loss: 0.5073736310005188--->Epoch: 879 | Loss: 0.5073736310005188--->Epoch: 880 | Loss: 0.5073736310005188--->Epoch: 881 | Loss: 0.5073736310005188--->Epoch: 882 | Loss: 0.5073736310005188--->Epoch: 883 | Loss: 0.5073736310005188--->Epoch: 884 | Loss: 0.5073736310005188--->Epoch: 885 | Loss: 0.5073736310005188--->Epoch: 886 | Loss: 0.5073736310005188--->Epoch: 887 | Loss: 0.5073736310005188--->Epoch: 888 | Loss: 0.5073736310005188--->Epoch: 889 | Loss: 0.5073736310005188--->Epoch: 890 | Loss: 0.5073736310005188--->Epoch: 891 | Loss: 0.5073736310005188--->Epoch: 892 | Loss: 0.5073736310005188--->Epoch: 893 | Loss: 0.5073736310005188--->Epoch: 894 | Loss: 0.5073736310005188--->Epoch: 895 | Loss: 0.5073736310005188--->Epoch: 896 | Loss: 0.5073736310005188--->Epoch: 897 | Loss: 0.5073736310005188--->Epoch: 898 | Loss: 0.5073736310005188--->Epoch: 899 | Loss: 0.5073736310005188--->Epoch: 900 | Loss: 0.5073736310005188--->\n",
            "Epoch: 901 | Loss: 0.5073736310005188--->Epoch: 902 | Loss: 0.5073736310005188--->Epoch: 903 | Loss: 0.5073736310005188--->Epoch: 904 | Loss: 0.5073736310005188--->Epoch: 905 | Loss: 0.5073736310005188--->Epoch: 906 | Loss: 0.5073736310005188--->Epoch: 907 | Loss: 0.5073736310005188--->Epoch: 908 | Loss: 0.5073736310005188--->Epoch: 909 | Loss: 0.5073736310005188--->Epoch: 910 | Loss: 0.5073736310005188--->Epoch: 911 | Loss: 0.5073736310005188--->Epoch: 912 | Loss: 0.5073736310005188--->Epoch: 913 | Loss: 0.5073736310005188--->Epoch: 914 | Loss: 0.5073736310005188--->Epoch: 915 | Loss: 0.5073736310005188--->Epoch: 916 | Loss: 0.5073736310005188--->Epoch: 917 | Loss: 0.5073736310005188--->Epoch: 918 | Loss: 0.5073736310005188--->Epoch: 919 | Loss: 0.5073736310005188--->Epoch: 920 | Loss: 0.5073736310005188--->Epoch: 921 | Loss: 0.5073736310005188--->Epoch: 922 | Loss: 0.5073736310005188--->Epoch: 923 | Loss: 0.5073736310005188--->Epoch: 924 | Loss: 0.5073736310005188--->Epoch: 925 | Loss: 0.5073736310005188--->Epoch: 926 | Loss: 0.5073736310005188--->Epoch: 927 | Loss: 0.5073736310005188--->Epoch: 928 | Loss: 0.5073736310005188--->Epoch: 929 | Loss: 0.5073736310005188--->Epoch: 930 | Loss: 0.5073736310005188--->Epoch: 931 | Loss: 0.5073736310005188--->Epoch: 932 | Loss: 0.5073736310005188--->Epoch: 933 | Loss: 0.5073736310005188--->Epoch: 934 | Loss: 0.5073736310005188--->Epoch: 935 | Loss: 0.5073736310005188--->Epoch: 936 | Loss: 0.5073736310005188--->Epoch: 937 | Loss: 0.5073736310005188--->Epoch: 938 | Loss: 0.5073736310005188--->Epoch: 939 | Loss: 0.5073736310005188--->Epoch: 940 | Loss: 0.5073736310005188--->Epoch: 941 | Loss: 0.5073736310005188--->Epoch: 942 | Loss: 0.5073736310005188--->Epoch: 943 | Loss: 0.5073736310005188--->Epoch: 944 | Loss: 0.5073736310005188--->Epoch: 945 | Loss: 0.5073736310005188--->Epoch: 946 | Loss: 0.5073736310005188--->Epoch: 947 | Loss: 0.5073736310005188--->Epoch: 948 | Loss: 0.5073736310005188--->Epoch: 949 | Loss: 0.5073736310005188--->Epoch: 950 | Loss: 0.5073736310005188--->Epoch: 951 | Loss: 0.5073736310005188--->Epoch: 952 | Loss: 0.5073736310005188--->Epoch: 953 | Loss: 0.5073736310005188--->Epoch: 954 | Loss: 0.5073736310005188--->Epoch: 955 | Loss: 0.5073736310005188--->Epoch: 956 | Loss: 0.5073736310005188--->Epoch: 957 | Loss: 0.5073736310005188--->Epoch: 958 | Loss: 0.5073736310005188--->Epoch: 959 | Loss: 0.5073736310005188--->Epoch: 960 | Loss: 0.5073736310005188--->Epoch: 961 | Loss: 0.5073736310005188--->Epoch: 962 | Loss: 0.5073736310005188--->Epoch: 963 | Loss: 0.5073736310005188--->Epoch: 964 | Loss: 0.5073736310005188--->Epoch: 965 | Loss: 0.5073736310005188--->Epoch: 966 | Loss: 0.5073736310005188--->Epoch: 967 | Loss: 0.5073736310005188--->Epoch: 968 | Loss: 0.5073736310005188--->Epoch: 969 | Loss: 0.5073736310005188--->Epoch: 970 | Loss: 0.5073736310005188--->Epoch: 971 | Loss: 0.5073736310005188--->Epoch: 972 | Loss: 0.5073736310005188--->Epoch: 973 | Loss: 0.5073736310005188--->Epoch: 974 | Loss: 0.5073736310005188--->Epoch: 975 | Loss: 0.5073736310005188--->Epoch: 976 | Loss: 0.5073736310005188--->Epoch: 977 | Loss: 0.5073736310005188--->Epoch: 978 | Loss: 0.5073736310005188--->Epoch: 979 | Loss: 0.5073736310005188--->Epoch: 980 | Loss: 0.5073736310005188--->Epoch: 981 | Loss: 0.5073736310005188--->Epoch: 982 | Loss: 0.5073736310005188--->Epoch: 983 | Loss: 0.5073736310005188--->Epoch: 984 | Loss: 0.5073736310005188--->Epoch: 985 | Loss: 0.5073736310005188--->Epoch: 986 | Loss: 0.5073736310005188--->Epoch: 987 | Loss: 0.5073736310005188--->Epoch: 988 | Loss: 0.5073736310005188--->Epoch: 989 | Loss: 0.5073736310005188--->Epoch: 990 | Loss: 0.5073736310005188--->Epoch: 991 | Loss: 0.5073736310005188--->Epoch: 992 | Loss: 0.5073736310005188--->Epoch: 993 | Loss: 0.5073736310005188--->Epoch: 994 | Loss: 0.5073736310005188--->Epoch: 995 | Loss: 0.5073736310005188--->Epoch: 996 | Loss: 0.5073736310005188--->Epoch: 997 | Loss: 0.5073736310005188--->Epoch: 998 | Loss: 0.5073736310005188--->Epoch: 999 | Loss: 0.5073736310005188--->Epoch: 1000 | Loss: 0.5073736310005188--->\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[ 0.1802,  0.1276,  0.1666, -0.0707, -0.1190,  0.1079, -0.1350,  0.0405,\n",
              "           0.0086,  0.0983,  0.1724,  0.0623,  0.0853,  0.1600, -0.0561, -0.0789,\n",
              "           0.1809,  0.0014, -0.0626, -0.1390,  0.0767,  0.1168, -0.0116, -0.1420,\n",
              "          -0.1257, -0.0787, -0.0462,  0.0834,  0.1705, -0.0364]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.1245], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  y_pred = model(X_test_tensor)\n",
        "  y_pred = (y_pred > 0.5).float()\n",
        "  accuracy = (y_pred == Y_test_tensor).float().mean()\n",
        "  print(f'Accuracy: {accuracy.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt4mYnxupi7i",
        "outputId": "ea719fec-df9e-4bf8-d6aa-d9a30b1c365e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5484764575958252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ZHqFjO8pvSM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}